{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 首先考虑离散的 State、Action 空间组成的Q函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Callable, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "State = int\n",
    "Action = int\n",
    "Reward = float\n",
    "ActionProbDistribution = List[float]\n",
    "\n",
    "class AbstractQFunc():\n",
    "    def get_value(self, state: State, action: Action) -> float:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def get_action_distribute(self, state: State) -> ActionProbDistribution:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_actions_count(self) -> int:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def set_value(self, state: State, action: Action, value: float) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class DiscreteQFunc(AbstractQFunc):\n",
    "    def __init__(self, state_nums: int, action_nums: int) -> None:\n",
    "        self._q_table = defaultdict(lambda : np.zeros(action_nums, dtype=np.float32))\n",
    "        self._state_nums = state_nums \n",
    "        self._action_nums = action_nums\n",
    "\n",
    "    def get_value(self, state, action) -> float:\n",
    "        return self._q_table[state][action]\n",
    "\n",
    "    def set_value(self, state: State, action: Action, value: float) -> None:\n",
    "        self._q_table[state][action] = value\n",
    "\n",
    "    def get_action_distribute(self, state: State) -> ActionProbDistribution:\n",
    "        return self._q_table[state]\n",
    "\n",
    "    def get_actions_count(self) -> int:\n",
    "        return self._action_nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 我们定义策略函数Pi(s) = P(a | s)；策略函数实际返回一个Action空间的分布函数，在离散的情况下，我们用一个数组表示这个分布， 下面定义一组函数，用于将Q转换为对应的策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 策略函数\n",
    "# todo: change the right type\n",
    "ActionProbDistribution = List[float]\n",
    "Strategy = Callable[[State], ActionProbDistribution]\n",
    "\n",
    "\n",
    "def to_strategy(f: AbstractQFunc) -> Strategy:\n",
    "    def _strategy(s: State) -> ActionProbDistribution:\n",
    "        return f.get_action_distribute(s)\n",
    "\n",
    "def to_strategy_epsilon_greedy(f: AbstractQFunc, epsilon: float) -> Strategy:\n",
    "    def _strategy(s: State) -> ActionProbDistribution:\n",
    "        # e-greedy 策略\n",
    "        if np.random.uniform(0, 1) > epsilon:\n",
    "            return f.get_action_distribute(s)\n",
    "        else:\n",
    "            # 随机选择动作 \n",
    "            return np.ones_like(f.get_actions_count(), dtype=np.float32) / f.get_actions_count()\n",
    "    return _strategy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最后是训练流程，在一个环境中，首先根据当前环境进行决策，再执行动作&观察反馈，最后根据信息更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractEnv():\n",
    "    # 如果返回的State部分是None，则表示Terminal 状态\n",
    "    def step(self, action: Action) ->  Tuple[Reward, Optional[State]]: \n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def reset(self) -> State:\n",
    "        return NotImplementedError()\n",
    "    \n",
    "\n",
    "class AbstractTrain():\n",
    "    def train(self):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们实现一个使用 epsilon-greedy 策略的Q-Learning 训练。（ps， 只针对离散的Q Learning）\n",
    "class QLearningTrain(AbstractTrain):\n",
    "    def __init__(self, gamma: float, learning_rate: float, epoch: int, epsilon_list: List[float],\n",
    "                 q_func: AbstractQFunc, env: AbstractEnv):\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epoch = epoch\n",
    "        self.epsilon_list = epsilon_list\n",
    "        \n",
    "        self.q_func = q_func\n",
    "        self.env = env\n",
    "        \n",
    "        self.current_state = None\n",
    "    def train(self):\n",
    "        for epoch in tqdm(range(self.epoch)):\n",
    "            init_state = self.env.reset()  # 回合开始前先重制环境          \n",
    "            self.current_state = init_state\n",
    "            while True: # 复杂的环境设置最大步数，也就是Horizon\n",
    "                # 获取此时Q 对应的epsilon-greedy 的策略 \n",
    "                e_greedy_s = to_strategy_epsilon_greedy(self.q_func, self.epsilon_list[epoch])\n",
    "                # 使用此时的策略进行决策\n",
    "                action = e_greedy_s(self.current_state)\n",
    "                # 执行此时的action\n",
    "                reward, next_state = self.env.step(action)\n",
    "                if next_state is None:\n",
    "                    # 达到terminal状态\n",
    "                    q_target = reward \n",
    "                else:\n",
    "                    q_target = reward + self.gamma * np.argmax(self.q_func.get_action_distribute(next_state))\n",
    "                \n",
    "                # 更新Q 函数\n",
    "                current_value = self.q_func.get_value(self.current_state, action)\n",
    "                self.q_func.set_value(self.current_state, action, \n",
    "                                       current_value + self.learning_rate * (q_target - current_value)\n",
    "                                    )\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
