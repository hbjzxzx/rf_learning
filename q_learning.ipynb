{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 首先考虑离散的 State、Action 空间组成的Q函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Callable, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "State = int\n",
    "Action = int\n",
    "Reward = float\n",
    "ActionProbDistribution = List[float]\n",
    "\n",
    "class AbstractQFunc():\n",
    "    def get_value(self, state: State, action: Action) -> float:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def get_action_distribute(self, state: State) -> ActionProbDistribution:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_actions_count(self) -> int:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def set_value(self, state: State, action: Action, value: float) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class DiscreteQFunc(AbstractQFunc):\n",
    "    def __init__(self, state_nums: int, action_nums: int) -> None:\n",
    "        self._q_table = defaultdict(lambda : np.zeros(action_nums, dtype=np.float32))\n",
    "        self._state_nums = state_nums \n",
    "        self._action_nums = action_nums\n",
    "\n",
    "    def get_value(self, state, action) -> float:\n",
    "        return self._q_table[state][action]\n",
    "\n",
    "    def set_value(self, state: State, action: Action, value: float) -> None:\n",
    "        self._q_table[state][action] = value\n",
    "\n",
    "    def get_action_distribute(self, state: State) -> ActionProbDistribution:\n",
    "        return self._q_table[state]\n",
    "\n",
    "    def get_actions_count(self) -> int:\n",
    "        return self._action_nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 我们定义策略函数Pi(s) = P(a | s)；策略函数实际返回一个Action空间的分布函数，在离散的情况下，我们用一个数组表示这个分布， 下面定义一组函数，用于将Q转换为对应的策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 策略函数\n",
    "# todo: change the right type\n",
    "ActionProbDistribution = List[float]\n",
    "Strategy = Callable[[State], ActionProbDistribution]\n",
    "\n",
    "\n",
    "def to_strategy(f: AbstractQFunc) -> Strategy:\n",
    "    def _strategy(s: State) -> ActionProbDistribution:\n",
    "        return f.get_action_distribute(s)\n",
    "\n",
    "def to_strategy_epsilon_greedy(f: AbstractQFunc, epsilon: float) -> Strategy:\n",
    "    def _strategy(s: State) -> ActionProbDistribution:\n",
    "        # e-greedy 策略\n",
    "        if np.random.uniform(0, 1) > epsilon:\n",
    "            return f.get_action_distribute(s)\n",
    "        else:\n",
    "            # 随机选择动作 \n",
    "            return np.ones_like(f.get_actions_count(), dtype=np.float32) / f.get_actions_count()\n",
    "    return _strategy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最后是训练流程，在一个环境中，首先根据当前环境进行决策，再执行动作&观察反馈，最后根据信息更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractEnv():\n",
    "    # 如果返回的State部分是None，则表示Terminal 状态\n",
    "    def step(self, action: Action) ->  Tuple[Reward, Optional[State]]: \n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def reset(self) -> State:\n",
    "        return NotImplementedError() \n",
    "    \n",
    "\n",
    "class AbstractTrainer():\n",
    "    def train(self):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "\n",
    "class AbstractTester():\n",
    "    def test(self):\n",
    "        raise NotImplementedError()\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们实现一个使用 epsilon-greedy 策略的Q-Learning 训练。（ps， 只针对离散的Q Learning）\n",
    "class QLearningTrainer(AbstractTrainer):\n",
    "    def __init__(self, gamma: float, learning_rate: float, epsilon_list: List[float],\n",
    "                 q_func: AbstractQFunc, env: AbstractEnv):\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon_list = epsilon_list\n",
    "        \n",
    "        self.q_func = q_func\n",
    "        self.env = env\n",
    "        \n",
    "        self.current_state = None\n",
    "\n",
    "        self.reward_record = []\n",
    "    def train(self, epoch_cnt: int, max_steps: int):\n",
    "        for epoch in tqdm(range(epoch_cnt)):\n",
    "            init_state = self.env.reset()  # 回合开始前先重制环境          \n",
    "            self.current_state = init_state\n",
    "            self.acc_reward = 0\n",
    "            # print(f'state change to: {self.current_state}')\n",
    "            for s in range(max_steps): # 复杂的环境设置最大步数，也就是Horizon\n",
    "                # 获取此时Q 对应的epsilon-greedy 的策略 \n",
    "                e_greedy_s = to_strategy_epsilon_greedy(self.q_func, self.epsilon_list[epoch])\n",
    "                # 使用此时的策略进行决策\n",
    "                action_dis = e_greedy_s(self.current_state)\n",
    "                action = np.argmax(action_dis) \n",
    "                # 执行此时的action\n",
    "                reward, next_state = self.env.step(action)\n",
    "                self.acc_reward += reward\n",
    "                if next_state is None:\n",
    "                    # 达到terminal状态\n",
    "                    q_target = reward \n",
    "                else:\n",
    "                    q_target = reward + self.gamma * np.argmax(self.q_func.get_action_distribute(next_state))\n",
    "                \n",
    "                # 更新Q 函数\n",
    "                current_value = self.q_func.get_value(self.current_state, action)\n",
    "                self.q_func.set_value(self.current_state, action, \n",
    "                                       current_value + self.learning_rate * (q_target - current_value)\n",
    "                                    )\n",
    "                self.current_state = next_state\n",
    "                # print(f'state change to: {self.current_state}')\n",
    "            self.reward_record.append(self.acc_reward)\n",
    "            # print(f'epoch total reward: {self.acc_reward}')\n",
    "\n",
    "\n",
    "class QFuncTester(AbstractTester):\n",
    "    def __init__(self, q_func: AbstractQFunc, env: AbstractEnv) -> None:\n",
    "        self._q_func = q_func\n",
    "        self._gym_env = env\n",
    "    \n",
    "    def test(self):\n",
    "        init_state = self.env.reset()  # 回合开始前先重制环境  \n",
    "        self.current_state = init_state \n",
    "        self.acc_reward = 0\n",
    "        greedy_strateggy = to_strategy(self._q_func)\n",
    "\n",
    "        while True:\n",
    "            action = greedy_strateggy(self.current_state) \n",
    "            np.argmax()\n",
    "            # do it\n",
    "            reward, next_state = self._gym_env.step(action)\n",
    "            self.acc_reward += reward\n",
    "            if next_state is None:\n",
    "                break\n",
    "            else:\n",
    "                self.current_state = next_state\n",
    "        print(f\"Test reward: {self.acc_reward}\")\n",
    "    \n",
    "\n",
    "    def test_batch(self):\n",
    "        ...\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env(AbstractEnv):\n",
    "    def __init__(self, gym_env: gym.Env):\n",
    "        self._gym_env = gym_env\n",
    "\n",
    "    def step(self, action: Action) ->  Tuple[Reward, Optional[State]]: \n",
    "        next_state, reward, is_terminated, is_truncated, _ = self._gym_env.step(action)\n",
    "        if is_terminated or is_truncated:\n",
    "            return reward, None\n",
    "        else:\n",
    "            return reward, next_state\n",
    "\n",
    "    def reset(self) -> State:\n",
    "        init_state, _ = self._gym_env.reset()\n",
    "        return init_state\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始使用Q-learning 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 121/50000 [00:00<01:22, 605.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [01:31<00:00, 548.27it/s]\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "\n",
    "GYM_ENV_NAME = 'CliffWalking-v0'\n",
    "_gym_env = gym.make(GYM_ENV_NAME)\n",
    "\n",
    "action_nums, state_nums = _gym_env.action_space.n, _gym_env.observation_space.n\n",
    "\n",
    "\n",
    "TRAIN_EPOCH = 50000\n",
    "LEARNING_RATE = 1e-2\n",
    "GAMMA = 0.9\n",
    "# EPSILON_LIST = [1.0 * 1.0/(i+1) for i in range(TRAIN_EPOCH)]\n",
    "EPSILON_LIST = [1.0 * 1.0/(i+1) for i in range(TRAIN_EPOCH)]\n",
    "\n",
    "q_func = DiscreteQFunc(state_nums=state_nums, action_nums=action_nums)\n",
    "env = Env(_gym_env)\n",
    "q_trainer = QLearningTrainer(\n",
    "    GAMMA,\n",
    "    LEARNING_RATE,\n",
    "    EPSILON_LIST,\n",
    "    q_func,\n",
    "    env\n",
    ")\n",
    "\n",
    "q_trainer.train(epoch_cnt=TRAIN_EPOCH, max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DiscreteQFunc' object has no attribute 'reward_record'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 绘制折线图\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mq_func\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreward_record\u001b[49m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 显示图形\u001b[39;00m\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DiscreteQFunc' object has no attribute 'reward_record'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 创建一个新的图形\n",
    "plt.figure()\n",
    "# 绘制折线图\n",
    "plt.plot(q_func.reward_record)\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
