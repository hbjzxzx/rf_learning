{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 首先考虑离散的 State、Action 空间组成的Q函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gymnasium'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[1;32m      9\u001b[0m State \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m\n\u001b[1;32m     10\u001b[0m Action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gymnasium'"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Callable, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "State = int\n",
    "Action = int\n",
    "Reward = float\n",
    "ActionProbDistribution = List[float]\n",
    "\n",
    "class AbstractQFunc():\n",
    "    def get_value(self, state: State, action: Action) -> float:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def get_action_distribute(self, state: State) -> ActionProbDistribution:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_actions_count(self) -> int:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def set_value(self, state: State, action: Action, value: float) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class DiscreteQFunc(AbstractQFunc):\n",
    "    def __init__(self, state_nums: int, action_nums: int) -> None:\n",
    "        self._q_table = defaultdict(lambda : np.zeros(action_nums, dtype=np.float32))\n",
    "        self._state_nums = state_nums \n",
    "        self._action_nums = action_nums\n",
    "\n",
    "    def get_value(self, state, action) -> float:\n",
    "        return self._q_table[state][action]\n",
    "\n",
    "    def set_value(self, state: State, action: Action, value: float) -> None:\n",
    "        self._q_table[state][action] = value\n",
    "\n",
    "    def get_action_distribute(self, state: State) -> ActionProbDistribution:\n",
    "        return self._q_table[state]\n",
    "\n",
    "    def get_actions_count(self) -> int:\n",
    "        return self._action_nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 我们定义策略函数Pi(s) = P(a | s)；策略函数实际返回一个Action空间的分布函数，在离散的情况下，我们用一个数组表示这个分布， 下面定义一组函数，用于将Q转换为对应的策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 策略函数\n",
    "# todo: change the right type\n",
    "ActionProbDistribution = List[float]\n",
    "Strategy = Callable[[State], ActionProbDistribution]\n",
    "\n",
    "\n",
    "def to_strategy(f: AbstractQFunc) -> Strategy:\n",
    "    def _strategy(s: State) -> ActionProbDistribution:\n",
    "        return f.get_action_distribute(s)\n",
    "\n",
    "def to_strategy_epsilon_greedy(f: AbstractQFunc, epsilon: float) -> Strategy:\n",
    "    def _strategy(s: State) -> ActionProbDistribution:\n",
    "        # e-greedy 策略\n",
    "        if np.random.uniform(0, 1) > epsilon:\n",
    "            return f.get_action_distribute(s)\n",
    "        else:\n",
    "            # 随机选择动作 \n",
    "            return np.ones_like(f.get_actions_count(), dtype=np.float32) / f.get_actions_count()\n",
    "    return _strategy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最后是训练流程，在一个环境中，首先根据当前环境进行决策，再执行动作&观察反馈，最后根据信息更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractEnv():\n",
    "    # 如果返回的State部分是None，则表示Terminal 状态\n",
    "    def step(self, action: Action) ->  Tuple[Reward, Optional[State]]: \n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def reset(self) -> State:\n",
    "        return NotImplementedError() \n",
    "    \n",
    "\n",
    "class AbstractTrainer():\n",
    "    def train(self):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "\n",
    "class AbstractTester():\n",
    "    def test(self):\n",
    "        raise NotImplementedError()\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们实现一个使用 epsilon-greedy 策略的Q-Learning 训练。（ps， 只针对离散的Q Learning）\n",
    "class QLearningTrain(AbstractTrainer):\n",
    "    def __init__(self, gamma: float, learning_rate: float, epoch: int, epsilon_list: List[float],\n",
    "                 q_func: AbstractQFunc, env: AbstractEnv):\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epoch = epoch\n",
    "        self.epsilon_list = epsilon_list\n",
    "        \n",
    "        self.q_func = q_func\n",
    "        self.env = env\n",
    "        \n",
    "        self.current_state = None\n",
    "    def train(self):\n",
    "        for epoch in tqdm(range(self.epoch)):\n",
    "            init_state = self.env.reset()  # 回合开始前先重制环境          \n",
    "            self.current_state = init_state\n",
    "            while True: # 复杂的环境设置最大步数，也就是Horizon\n",
    "                # 获取此时Q 对应的epsilon-greedy 的策略 \n",
    "                e_greedy_s = to_strategy_epsilon_greedy(self.q_func, self.epsilon_list[epoch])\n",
    "                # 使用此时的策略进行决策\n",
    "                action = e_greedy_s(self.current_state)\n",
    "                # 执行此时的action\n",
    "                reward, next_state = self.env.step(action)\n",
    "                if next_state is None:\n",
    "                    # 达到terminal状态\n",
    "                    q_target = reward \n",
    "                else:\n",
    "                    q_target = reward + self.gamma * np.argmax(self.q_func.get_action_distribute(next_state))\n",
    "                \n",
    "                # 更新Q 函数\n",
    "                current_value = self.q_func.get_value(self.current_state, action)\n",
    "                self.q_func.set_value(self.current_state, action, \n",
    "                                       current_value + self.learning_rate * (q_target - current_value)\n",
    "                                    )\n",
    "\n",
    "\n",
    "class QFuncTester(AbstractEnv):\n",
    "    def __init__(self, q_func: AbstractQFunc, env: AbstractEnv, epoch: int) -> None:\n",
    "        self._q_func = q_func\n",
    "        self._gym_env = env\n",
    "        self._epoch = epoch\n",
    "    \n",
    "    def test(self):\n",
    "        for epoch in tqdm(range(self._epoch)):\n",
    "            init_state = self.env.reset()  # 回合开始前先重制环境  \n",
    "            self.current_state = init_state \n",
    "            while True:\n",
    "                greedy_strateggy = to_strategy(self._q_func)\n",
    "                action = greedy_strateggy(self.current_state) \n",
    "                # do it\n",
    "                reward, next_state = self._gym_env.step(action)\n",
    "                if next_state is None:\n",
    "                    break\n",
    "                eles:\n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mEnv\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgym_env\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEnv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gym_env\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgym_env\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m, in \u001b[0;36mEnv\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mEnv\u001b[39;00m():\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gym_env: \u001b[43mgym\u001b[49m\u001b[38;5;241m.\u001b[39mEnv):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gym_env \u001b[38;5;241m=\u001b[39m gym_env\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: Action) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m  Tuple[Reward, Optional[State]]: \n",
      "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "class Env():\n",
    "    def __init__(self, gym_env: gym.Env):\n",
    "        self._gym_env = gym_env\n",
    "\n",
    "    def step(self, action: Action) ->  Tuple[Reward, Optional[State]]: \n",
    "        next_state, reward, is_terminated, is_truncated = self._gym_env.step(action)\n",
    "        if is_terminated or is_truncated:\n",
    "            return reward, None\n",
    "        else:\n",
    "            return reward, next_state\n",
    "\n",
    "    def reset(self) -> State:\n",
    "        init_state, _ = self._gym_env.reset()\n",
    "        return init_state\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始使用Q-learning 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GYM_ENV_NAME = 'CliffWalking-v0'\n",
    "_gym_env = gym.make(GYM_ENV_NAME)\n",
    "\n",
    "action_nums, state_nums = _gym_env.action_space.n, _gym_env.observation_space.n\n",
    "\n",
    "\n",
    "TRAIN_EPOCH = 400\n",
    "LEARNING_RATE = 1e-5\n",
    "GAMMA = 0.9\n",
    "EPSILON_LIST = [1e-2 * 1.0/(i+1) for i in range(TRAIN_EPOCH)]\n",
    "\n",
    "q_func = DiscreteQFunc(state_nums=state_nums, action_nums=action_nums)\n",
    "QLearningTrain(\n",
    "    GAMMA,\n",
    "    LEARNING_RATE,\n",
    "    TRAIN_EPOCH,\n",
    "    EPSILON_LIST,\n",
    "    q_func,\n",
    "    _gym_env\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
