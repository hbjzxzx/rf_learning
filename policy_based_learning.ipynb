{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "from policy_based import PolicyNetFunc, PolicyNetTrainer, PolicyNetTester\n",
    "from deep_q import Discrete1ContinuousAction\n",
    "from env import Env, get_action_discreter\n",
    "from utils import clear_target_path, show_gif_on_jupyternb, to_gif\n",
    "from train_test_util import start_test, start_train, StandarTestProcess, StandarTrainProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab1 Policy-Base Train\n",
    "\n",
    "使用CarPole-V1 环境，测试Policy-Based REINFORCE 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GYM_ENV_NAME = 'CartPole-v1'\n",
    "env = Env.from_env_name(GYM_ENV_NAME)\n",
    "RESULT_DIR_NAME='cartpoleV1'\n",
    "\n",
    "LOG_PATH = Path(f'./run/logs/{RESULT_DIR_NAME}/REINFORCE')\n",
    "MODEL_PATH = Path(f'./run/model/{RESULT_DIR_NAME}/REINFORCE.pth')\n",
    "TEST_OUTPUT_PATH = Path(f'./run/test_result/{RESULT_DIR_NAME}/REINFORCE')\n",
    "\n",
    "# 打印查看环境的动作空间和状态空间 \n",
    "env.print_state_action_dims()\n",
    "\n",
    "\n",
    "TRAIN_EPOCH = 1000\n",
    "HIDDEN_DIM = 256\n",
    "LEARNING_RATE = 2e-3\n",
    "GAMMA = 0.99\n",
    "\n",
    "_USE_CUDA = True and torch.cuda.is_available()\n",
    "# _USE_CUDA = False and torch.cuda.is_available()\n",
    "\n",
    "policy_func = PolicyNetFunc(env.get_state_dim()[0], \n",
    "                   env.get_action_dim()[0], \n",
    "                   hidden_dim=HIDDEN_DIM, \n",
    "                   device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "\n",
    "policy_func_trainer = PolicyNetTrainer(policy_func=policy_func,\n",
    "                                  env=env,\n",
    "                                  learning_rate=LEARNING_RATE,\n",
    "                                  gamma=GAMMA,\n",
    "                                  logger_folder=LOG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_train(StandarTrainProcess(\n",
    "    trainer=policy_func_trainer,\n",
    "    model=policy_func,\n",
    "    train_epoch=TRAIN_EPOCH,\n",
    "    log_path=LOG_PATH,\n",
    "    model_path=MODEL_PATH\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_policy_func = PolicyNetFunc.from_file(MODEL_PATH)\n",
    "_render_env = Env(gym.make(GYM_ENV_NAME, render_mode='rgb_array_list'))\n",
    "\n",
    "policy_func_tester = PolicyNetTester(\n",
    "    policy_fun=test_policy_func,\n",
    "    env=_render_env\n",
    ")\n",
    "\n",
    "\n",
    "start_test(\n",
    "    StandarTestProcess(\n",
    "        model=test_policy_func,\n",
    "        tester=policy_func_tester,\n",
    "        env=_render_env,\n",
    "        test_output_path=TEST_OUTPUT_PATH,\n",
    "        test_epoch=1000,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Pendulum-v1 环境，测试Policy-Based REINFORCE 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GYM_ENV_NAME = 'Pendulum-v1'\n",
    "RESULT_DIR_NAME = 'pendulumV1'\n",
    "\n",
    "env = Env.from_env_name(GYM_ENV_NAME)\n",
    "\n",
    "LOG_PATH = Path(f'./run/logs/{RESULT_DIR_NAME}/policy_based')\n",
    "MODEL_PATH = Path(f'./run/model/{RESULT_DIR_NAME}/policy_model.pth')\n",
    "TEST_OUTPUT_PATH = Path(f'./run/test_result/{RESULT_DIR_NAME}_policy')\n",
    "\n",
    "# 打印查看环境的动作空间和状态空间 \n",
    "env.print_state_action_dims()\n",
    "\n",
    "# 动作空间离散化程度（用11个区间来替代连续动作空间）\n",
    "BINS = 11\n",
    "\n",
    "TRAIN_EPOCH = 1000\n",
    "HIDDEN_DIM = 512\n",
    "LEARNING_RATE = 1e-3\n",
    "GAMMA = 0.99\n",
    "\n",
    "_USE_CUDA = True and torch.cuda.is_available()\n",
    "# _USE_CUDA = False and torch.cuda.is_available()\n",
    "\n",
    "policy_func = PolicyNetFunc(env.get_state_dim()[0], \n",
    "                   action_nums=BINS, \n",
    "                   hidden_dim=HIDDEN_DIM, \n",
    "                   device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "\n",
    "policy_func_trainer = PolicyNetTrainer(policy_func=policy_func,\n",
    "                                  env=env,\n",
    "                                  learning_rate=LEARNING_RATE,\n",
    "                                  gamma=GAMMA,\n",
    "                                  logger_folder=LOG_PATH,\n",
    "                                  action_converter=get_action_discreter(env, BINS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_train(StandarTrainProcess(\n",
    "    trainer=policy_func_trainer,\n",
    "    model=policy_func,\n",
    "    train_epoch=TRAIN_EPOCH,\n",
    "    log_path=LOG_PATH,\n",
    "    model_path=MODEL_PATH\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_policy_func = PolicyNetFunc.from_file(MODEL_PATH)\n",
    "_render_env = Env(gym.make(GYM_ENV_NAME, render_mode='rgb_array_list'))\n",
    "\n",
    "policy_func_tester = PolicyNetTester(\n",
    "    policy_fun=test_policy_func,\n",
    "    env=_render_env,\n",
    "    action_converter=get_action_discreter(env, BINS)\n",
    ")\n",
    "\n",
    "\n",
    "start_test(\n",
    "    StandarTestProcess(\n",
    "        model=test_policy_func,\n",
    "        tester=policy_func_tester,\n",
    "        env=_render_env,\n",
    "        test_output_path=TEST_OUTPUT_PATH,\n",
    "        test_epoch=1000,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Pendulum-v1 环境，测试Policy-Based AC 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[36menv\u001b[0m: - \u001b[1maction: Box(-2.0, 2.0, (1,), float32), space: Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)\u001b[0m; \u001b[32m2024-06-25 09:16:25\u001b[0m \u001b[36mprint_state_action_dims\u001b[0m:\u001b[36m68\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from policy_based import PolicyValueNetTrainer, ActionStateValueNetFunc\n",
    "\n",
    "GYM_ENV_NAME = 'Pendulum-v1'\n",
    "RESULT_DIR_NAME = 'pendulumV1'\n",
    "\n",
    "env = Env.from_env_name(GYM_ENV_NAME)\n",
    "\n",
    "LOG_PATH = Path(f'./run/logs/{RESULT_DIR_NAME}/AC')\n",
    "MODEL_PATH = Path(f'./run/model/{RESULT_DIR_NAME}/AC/AC.pth')\n",
    "TEST_OUTPUT_PATH = Path(f'./run/test_result/{RESULT_DIR_NAME}/AC')\n",
    "\n",
    "# 打印查看环境的动作空间和状态空间 \n",
    "env.print_state_action_dims()\n",
    "\n",
    "# 动作空间离散化程度（用11个区间来替代连续动作空间）\n",
    "BINS = 11\n",
    "\n",
    "TRAIN_EPOCH = 1000\n",
    "HIDDEN_DIM = 512\n",
    "LEARNING_RATE = 1e-3\n",
    "GAMMA = 0.99\n",
    "\n",
    "_USE_CUDA = True and torch.cuda.is_available()\n",
    "# _USE_CUDA = False and torch.cuda.is_available()\n",
    "\n",
    "policy_func = PolicyNetFunc(env.get_state_dim()[0], \n",
    "                   action_nums=BINS, \n",
    "                   hidden_dim=HIDDEN_DIM, \n",
    "                   device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "value_func = ActionStateValueNetFunc(env.get_state_dim()[0],\n",
    "                          action_nums=BINS,\n",
    "                          hidden_dim=HIDDEN_DIM,\n",
    "                          device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "\n",
    "policy_func_trainer = PolicyValueNetTrainer(\n",
    "                                  policy_func=policy_func,\n",
    "                                  value_func=value_func,\n",
    "                                  env=env,\n",
    "                                  learning_rate=LEARNING_RATE,\n",
    "                                  gamma=GAMMA,\n",
    "                                  logger_folder=LOG_PATH,\n",
    "                                  action_converter=get_action_discreter(env, BINS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_target_path(LOG_PATH)\n",
    "clear_target_path(MODEL_PATH)\n",
    "print(f'start training, now datetime: {datetime.datetime.now()}')\n",
    "policy_func_trainer.train(train_epoch=TRAIN_EPOCH)\n",
    "print(f'end training, saving model to: {MODEL_PATH}, now datetime: {datetime.datetime.now()}')\n",
    "\n",
    "policy_func.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_train(\n",
    "    StandarTrainProcess(\n",
    "        trainer=policy_func_trainer,\n",
    "        model=policy_func,\n",
    "        train_epoch=TRAIN_EPOCH,\n",
    "        log_path=LOG_PATH,\n",
    "        model_path=MODEL_PATH\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
