{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "from policy_based import PolicyNetFunc, PolicyNetTrainer, PolicyNetTester, PolicyNetTrainerWithBase, ValueNetFunc\n",
    "from deep_q import Discrete1ContinuousAction\n",
    "from env import Env, get_action_discreter\n",
    "from utils import clear_target_path, show_gif_on_jupyternb, to_gif\n",
    "from train_test_util import start_test, start_train, StandarTestProcess, StandarTrainProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用CarPole-V1 环境，测试Policy-Based REINFORCE 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GYM_ENV_NAME = 'CartPole-v1'\n",
    "env = Env.from_env_name(GYM_ENV_NAME)\n",
    "RESULT_DIR_NAME='cartpoleV1'\n",
    "\n",
    "LOG_PATH = Path(f'./run/logs/{RESULT_DIR_NAME}/REINFORCE')\n",
    "MODEL_PATH = Path(f'./run/model/{RESULT_DIR_NAME}/REINFORCE.pth')\n",
    "TEST_OUTPUT_PATH = Path(f'./run/test_result/{RESULT_DIR_NAME}/REINFORCE')\n",
    "\n",
    "# 打印查看环境的动作空间和状态空间 \n",
    "env.print_state_action_dims()\n",
    "\n",
    "\n",
    "TRAIN_EPOCH = 1000\n",
    "HIDDEN_DIM = 256\n",
    "LEARNING_RATE = 2e-3\n",
    "GAMMA = 0.99\n",
    "\n",
    "_USE_CUDA = True and torch.cuda.is_available()\n",
    "# _USE_CUDA = False and torch.cuda.is_available()\n",
    "\n",
    "policy_func = PolicyNetFunc(env.get_state_dim()[0], \n",
    "                   env.get_action_dim()[0], \n",
    "                   hidden_dim=HIDDEN_DIM, \n",
    "                   device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "\n",
    "policy_func_trainer = PolicyNetTrainer(policy_func=policy_func,\n",
    "                                  env=env,\n",
    "                                  learning_rate=LEARNING_RATE,\n",
    "                                  gamma=GAMMA,\n",
    "                                  logger_folder=LOG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_train(StandarTrainProcess(\n",
    "    trainer=policy_func_trainer,\n",
    "    model=policy_func,\n",
    "    train_epoch=TRAIN_EPOCH,\n",
    "    log_path=LOG_PATH,\n",
    "    model_path=MODEL_PATH\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/gif": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_policy_func = PolicyNetFunc.from_file(MODEL_PATH)\n",
    "_render_env = Env(gym.make(GYM_ENV_NAME, render_mode='rgb_array_list'))\n",
    "\n",
    "policy_func_tester = PolicyNetTester(\n",
    "    policy_fun=test_policy_func,\n",
    "    env=_render_env\n",
    ")\n",
    "\n",
    "\n",
    "start_test(\n",
    "    StandarTestProcess(\n",
    "        model=test_policy_func,\n",
    "        tester=policy_func_tester,\n",
    "        env=_render_env,\n",
    "        test_output_path=TEST_OUTPUT_PATH,\n",
    "        test_epoch=1000,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用CarPole-V1 环境，测试Policy-Based + 基线的 REINFORCE 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[36menv\u001b[0m: - \u001b[1maction: 2, space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\u001b[0m; \u001b[32m2024-07-09 01:29:40\u001b[0m \u001b[36mprint_state_action_dims\u001b[0m:\u001b[36m68\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "GYM_ENV_NAME = 'CartPole-v1'\n",
    "RESULT_DIR_NAME='cartpoleV1'\n",
    "env = Env.from_env_name(GYM_ENV_NAME)\n",
    "\n",
    "LOG_PATH = Path(f'./run/logs/{RESULT_DIR_NAME}/REINFORCE_With_BASE')\n",
    "MODEL_PATH = Path(f'./run/model/{RESULT_DIR_NAME}/REINFORCE_With_BASE.pth')\n",
    "TEST_OUTPUT_PATH = Path(f'./run/test_result/{RESULT_DIR_NAME}/REINFORCE_With_BASE')\n",
    "\n",
    "# 打印查看环境的动作空间和状态空间 \n",
    "env.print_state_action_dims()\n",
    "\n",
    "\n",
    "TRAIN_EPOCH = 1000\n",
    "HIDDEN_DIM = 256\n",
    "LEARNING_RATE = 2e-3\n",
    "VALUE_LEARNING_RATE = 1e-3\n",
    "GAMMA = 0.99\n",
    "\n",
    "_USE_CUDA = True and torch.cuda.is_available()\n",
    "# _USE_CUDA = False and torch.cuda.is_available()\n",
    "\n",
    "value_func = ValueNetFunc(\n",
    "                env.get_state_dim()[0], \n",
    "                hidden_dim=HIDDEN_DIM, \n",
    "                device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "policy_func = PolicyNetFunc(env.get_state_dim()[0], \n",
    "                   env.get_action_dim()[0], \n",
    "                   hidden_dim=HIDDEN_DIM, \n",
    "                   device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "\n",
    "policy_func_trainer = PolicyNetTrainerWithBase(\n",
    "                                  policy_func=policy_func,\n",
    "                                  value_func=value_func,\n",
    "                                  value_learning_rate=LEARNING_RATE,\n",
    "                                  env=env,\n",
    "                                  learning_rate=VALUE_LEARNING_RATE,\n",
    "                                  gamma=GAMMA,\n",
    "                                  logger_folder=LOG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtrain_test_util\u001b[0m: - \u001b[1mstart training, now datetime: 2024-07-09 01:29:40.535512\u001b[0m; \u001b[32m2024-07-09 01:29:40\u001b[0m \u001b[36mstart_train\u001b[0m:\u001b[36m62\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtrain_test_util\u001b[0m: - \u001b[1mFirst, clean log path: run\\logs\\cartpoleV1\\REINFORCE_With_BASE, and clean model path: run\\model\\cartpoleV1\\REINFORCE_With_BASE.pth\u001b[0m; \u001b[32m2024-07-09 01:29:40\u001b[0m \u001b[36mstart_train\u001b[0m:\u001b[36m63\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtrain_test_util\u001b[0m: - \u001b[1mtrain started\u001b[0m; \u001b[32m2024-07-09 01:29:40\u001b[0m \u001b[36mstart_train\u001b[0m:\u001b[36m68\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 53/1000 [00:01<00:17, 54.88it/s, reward=35.00, step=35]"
     ]
    }
   ],
   "source": [
    "start_train(StandarTrainProcess(\n",
    "    trainer=policy_func_trainer,\n",
    "    model=policy_func,\n",
    "    train_epoch=TRAIN_EPOCH,\n",
    "    log_path=LOG_PATH,\n",
    "    model_path=MODEL_PATH\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_policy_func = PolicyNetFunc.from_file(MODEL_PATH)\n",
    "_render_env = Env(gym.make(GYM_ENV_NAME, render_mode='rgb_array_list'))\n",
    "\n",
    "policy_func_tester = PolicyNetTester(\n",
    "    policy_fun=test_policy_func,\n",
    "    env=_render_env\n",
    ")\n",
    "\n",
    "\n",
    "start_test(\n",
    "    StandarTestProcess(\n",
    "        model=test_policy_func,\n",
    "        tester=policy_func_tester,\n",
    "        env=_render_env,\n",
    "        test_output_path=TEST_OUTPUT_PATH,\n",
    "        test_epoch=1000,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Pendulum-v1 环境，测试Policy-Based REINFORCE 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GYM_ENV_NAME = 'Pendulum-v1'\n",
    "RESULT_DIR_NAME = 'pendulumV1'\n",
    "\n",
    "env = Env.from_env_name(GYM_ENV_NAME)\n",
    "\n",
    "LOG_PATH = Path(f'./run/logs/{RESULT_DIR_NAME}/policy_based')\n",
    "MODEL_PATH = Path(f'./run/model/{RESULT_DIR_NAME}/policy_model.pth')\n",
    "TEST_OUTPUT_PATH = Path(f'./run/test_result/{RESULT_DIR_NAME}_policy')\n",
    "\n",
    "# 打印查看环境的动作空间和状态空间 \n",
    "env.print_state_action_dims()\n",
    "\n",
    "# 动作空间离散化程度（用11个区间来替代连续动作空间）\n",
    "BINS = 11\n",
    "\n",
    "TRAIN_EPOCH = 1000\n",
    "HIDDEN_DIM = 512\n",
    "LEARNING_RATE = 1e-3\n",
    "GAMMA = 0.99\n",
    "\n",
    "_USE_CUDA = True and torch.cuda.is_available()\n",
    "# _USE_CUDA = False and torch.cuda.is_available()\n",
    "\n",
    "policy_func = PolicyNetFunc(env.get_state_dim()[0], \n",
    "                   action_nums=BINS, \n",
    "                   hidden_dim=HIDDEN_DIM, \n",
    "                   device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "\n",
    "policy_func_trainer = PolicyNetTrainer(policy_func=policy_func,\n",
    "                                  env=env,\n",
    "                                  learning_rate=LEARNING_RATE,\n",
    "                                  gamma=GAMMA,\n",
    "                                  logger_folder=LOG_PATH,\n",
    "                                  action_converter=get_action_discreter(env, BINS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_train(StandarTrainProcess(\n",
    "    trainer=policy_func_trainer,\n",
    "    model=policy_func,\n",
    "    train_epoch=TRAIN_EPOCH,\n",
    "    log_path=LOG_PATH,\n",
    "    model_path=MODEL_PATH\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_policy_func = PolicyNetFunc.from_file(MODEL_PATH)\n",
    "_render_env = Env(gym.make(GYM_ENV_NAME, render_mode='rgb_array_list'))\n",
    "\n",
    "policy_func_tester = PolicyNetTester(\n",
    "    policy_fun=test_policy_func,\n",
    "    env=_render_env,\n",
    "    action_converter=get_action_discreter(env, BINS)\n",
    ")\n",
    "\n",
    "\n",
    "start_test(\n",
    "    StandarTestProcess(\n",
    "        model=test_policy_func,\n",
    "        tester=policy_func_tester,\n",
    "        env=_render_env,\n",
    "        test_output_path=TEST_OUTPUT_PATH,\n",
    "        test_epoch=1000,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Pendulum-v1 环境，测试Policy-Based + 基线的 REINFORCE 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GYM_ENV_NAME = 'Pendulum-v1'\n",
    "RESULT_DIR_NAME = 'pendulumV1'\n",
    "\n",
    "env = Env.from_env_name(GYM_ENV_NAME)\n",
    "\n",
    "LOG_PATH = Path(f'./run/logs/{RESULT_DIR_NAME}/REINFORCE_With_BASE')\n",
    "MODEL_PATH = Path(f'./run/model/{RESULT_DIR_NAME}/REINFORCE_With_BASE.pth')\n",
    "TEST_OUTPUT_PATH = Path(f'./run/test_result/{RESULT_DIR_NAME}/REINFORCE_With_BASE')\n",
    "\n",
    "# 打印查看环境的动作空间和状态空间 \n",
    "env.print_state_action_dims()\n",
    "\n",
    "# 动作空间离散化程度（用11个区间来替代连续动作空间）\n",
    "BINS = 11\n",
    "\n",
    "TRAIN_EPOCH = 3000\n",
    "HIDDEN_DIM = 512\n",
    "LEARNING_RATE = 5e-4\n",
    "VALUE_LEARNING_RATE = 5e-3\n",
    "GAMMA = 0.999\n",
    "\n",
    "_USE_CUDA = True and torch.cuda.is_available()\n",
    "# _USE_CUDA = False and torch.cuda.is_available()\n",
    "\n",
    "value_func = ValueNetFunc(\n",
    "                env.get_state_dim()[0], \n",
    "                hidden_dim=HIDDEN_DIM, \n",
    "                device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "policy_func = PolicyNetFunc(env.get_state_dim()[0], \n",
    "                   action_nums=BINS, \n",
    "                   hidden_dim=HIDDEN_DIM, \n",
    "                   device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "\n",
    "policy_func_trainer = PolicyNetTrainerWithBase(\n",
    "                                  policy_func=policy_func,\n",
    "                                  value_func=value_func,\n",
    "                                  value_learning_rate=VALUE_LEARNING_RATE,\n",
    "                                  env=env,\n",
    "                                  learning_rate=LEARNING_RATE,\n",
    "                                  gamma=GAMMA,\n",
    "                                  logger_folder=LOG_PATH,\n",
    "                                  action_converter=get_action_discreter(env, BINS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_train(StandarTrainProcess(\n",
    "    trainer=policy_func_trainer,\n",
    "    model=policy_func,\n",
    "    train_epoch=TRAIN_EPOCH,\n",
    "    log_path=LOG_PATH,\n",
    "    model_path=MODEL_PATH\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_policy_func = PolicyNetFunc.from_file(MODEL_PATH)\n",
    "_render_env = Env(gym.make(GYM_ENV_NAME, render_mode='rgb_array_list'))\n",
    "\n",
    "policy_func_tester = PolicyNetTester(\n",
    "    policy_fun=test_policy_func,\n",
    "    env=_render_env,\n",
    "    action_converter=get_action_discreter(_render_env, BINS),\n",
    "    stochastic=True\n",
    ")\n",
    "\n",
    "\n",
    "start_test(\n",
    "    StandarTestProcess(\n",
    "        model=test_policy_func,\n",
    "        tester=policy_func_tester,\n",
    "        env=_render_env,\n",
    "        test_output_path=TEST_OUTPUT_PATH,\n",
    "        test_epoch=1000,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用CarPole-V1 环境，测试Policy-Based AC 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from policy_based import PolicyValueNetTrainer, ActionStateValueNetFunc\n",
    "\n",
    "GYM_ENV_NAME = 'CartPole-v1'\n",
    "RESULT_DIR_NAME='cartpoleV1'\n",
    "env = Env.from_env_name(GYM_ENV_NAME)\n",
    "\n",
    "LOG_PATH = Path(f'./run/logs/{RESULT_DIR_NAME}/AC')\n",
    "MODEL_PATH = Path(f'./run/model/{RESULT_DIR_NAME}/AC/AC.pth')\n",
    "TEST_OUTPUT_PATH = Path(f'./run/test_result/{RESULT_DIR_NAME}/AC')\n",
    "\n",
    "# 打印查看环境的动作空间和状态空间 \n",
    "env.print_state_action_dims()\n",
    "\n",
    "TRAIN_EPOCH = 3000\n",
    "HIDDEN_DIM = 256\n",
    "LEARNING_RATE = 2e-3\n",
    "VLEARNING_RATE = 5e-3\n",
    "GAMMA = 0.99\n",
    "\n",
    "_USE_CUDA = True and torch.cuda.is_available()\n",
    "# _USE_CUDA = False and torch.cuda.is_available()\n",
    "\n",
    "policy_func = PolicyNetFunc(env.get_state_dim()[0], \n",
    "                   action_nums=env.get_action_dim()[0], \n",
    "                   hidden_dim=HIDDEN_DIM * 2, \n",
    "                   device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "value_func = ActionStateValueNetFunc(env.get_state_dim()[0],\n",
    "                          action_nums=env.get_action_dim()[0],\n",
    "                          hidden_dim=HIDDEN_DIM,\n",
    "                          device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "\n",
    "policy_func_trainer = PolicyValueNetTrainer(\n",
    "                                  policy_func=policy_func,\n",
    "                                  value_func=value_func,\n",
    "                                  vlearning_rate=VLEARNING_RATE,\n",
    "                                  env=env,\n",
    "                                  learning_rate=LEARNING_RATE,\n",
    "                                  gamma=GAMMA,\n",
    "                                  logger_folder=LOG_PATH\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_train(\n",
    "    StandarTrainProcess(\n",
    "        trainer=policy_func_trainer,\n",
    "        model=policy_func,\n",
    "        train_epoch=TRAIN_EPOCH,\n",
    "        log_path=LOG_PATH,\n",
    "        model_path=MODEL_PATH\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_policy_func = PolicyNetFunc.from_file(MODEL_PATH)\n",
    "_render_env = Env(gym.make(GYM_ENV_NAME, render_mode='rgb_array_list'))\n",
    "\n",
    "policy_func_tester = PolicyNetTester(\n",
    "    policy_fun=test_policy_func,\n",
    "    env=_render_env\n",
    ")\n",
    "\n",
    "\n",
    "avg_reward = start_test(\n",
    "    StandarTestProcess(\n",
    "        model=test_policy_func,\n",
    "        tester=policy_func_tester,\n",
    "        env=_render_env,\n",
    "        test_output_path=TEST_OUTPUT_PATH,\n",
    "        test_epoch=100,\n",
    "        show_result=False\n",
    "    )\n",
    ")\n",
    "print(f'avg reward: {avg_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用CarPole-V1 环境，测试Policy-Based TRPO 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[36menv\u001b[0m: - \u001b[1maction: 2, space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\u001b[0m; \u001b[32m2024-07-09 01:43:19\u001b[0m \u001b[36mprint_state_action_dims\u001b[0m:\u001b[36m68\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from policy_based import PolicyNetTrainerWithTRPO, ActionStateValueNetFunc\n",
    "\n",
    "GYM_ENV_NAME = 'CartPole-v1'\n",
    "RESULT_DIR_NAME='cartpoleV1'\n",
    "env = Env.from_env_name(GYM_ENV_NAME)\n",
    "\n",
    "LOG_PATH = Path(f'./run/logs/{RESULT_DIR_NAME}/AC')\n",
    "MODEL_PATH = Path(f'./run/model/{RESULT_DIR_NAME}/AC/AC.pth')\n",
    "TEST_OUTPUT_PATH = Path(f'./run/test_result/{RESULT_DIR_NAME}/AC')\n",
    "\n",
    "# 打印查看环境的动作空间和状态空间 \n",
    "env.print_state_action_dims()\n",
    "\n",
    "TRAIN_EPOCH = 3000\n",
    "HIDDEN_DIM = 256\n",
    "LEARNING_RATE = 2e-3\n",
    "VLEARNING_RATE = 5e-3\n",
    "GAMMA = 0.99\n",
    "\n",
    "_USE_CUDA = True and torch.cuda.is_available()\n",
    "# _USE_CUDA = False and torch.cuda.is_available()\n",
    "\n",
    "policy_func = PolicyNetFunc(env.get_state_dim()[0], \n",
    "                   action_nums=env.get_action_dim()[0], \n",
    "                   hidden_dim=HIDDEN_DIM * 2, \n",
    "                   device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "value_func = ActionStateValueNetFunc(env.get_state_dim()[0],\n",
    "                          action_nums=env.get_action_dim()[0],\n",
    "                          hidden_dim=HIDDEN_DIM,\n",
    "                          device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "\n",
    "policy_func_trainer = PolicyNetTrainerWithTRPO(\n",
    "                                  policy_func=policy_func,\n",
    "                                  value_func=value_func,\n",
    "                                  value_learning_rate=VLEARNING_RATE,\n",
    "                                  env=env,\n",
    "                                  learning_rate=LEARNING_RATE,\n",
    "                                  gamma=GAMMA,\n",
    "                                  logger_folder=LOG_PATH,\n",
    "                                  kl_threadhold=0.0005,\n",
    "                                  search_alpha=0.5\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtrain_test_util\u001b[0m: - \u001b[1mstart training, now datetime: 2024-07-09 01:43:20.898569\u001b[0m; \u001b[32m2024-07-09 01:43:20\u001b[0m \u001b[36mstart_train\u001b[0m:\u001b[36m62\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtrain_test_util\u001b[0m: - \u001b[1mFirst, clean log path: run\\logs\\cartpoleV1\\AC, and clean model path: run\\model\\cartpoleV1\\AC\\AC.pth\u001b[0m; \u001b[32m2024-07-09 01:43:20\u001b[0m \u001b[36mstart_train\u001b[0m:\u001b[36m63\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtrain_test_util\u001b[0m: - \u001b[1mtrain started\u001b[0m; \u001b[32m2024-07-09 01:43:20\u001b[0m \u001b[36mstart_train\u001b[0m:\u001b[36m68\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x ini: :  tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "kl_divergence is:  tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "kl_grad_vector is:  tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', grad_fn=<CatBackward0>)\n",
      "hp at 0 is:  tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "x at 0 is:  tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\n",
      "kl_divergence is:  tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "kl_grad_vector is:  tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', grad_fn=<CatBackward0>)\n",
      "hp at 1 is:  tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\n",
      "x at 1 is:  tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\n",
      "kl_divergence is:  tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "kl_grad_vector is:  tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', grad_fn=<CatBackward0>)\n",
      "hp at 2 is:  tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\n",
      "x at 2 is:  tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\n",
      "kl_divergence is:  tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "kl_grad_vector is:  tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', grad_fn=<CatBackward0>)\n",
      "hp at 3 is:  tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\n",
      "x at 3 is:  tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\n",
      "kl_divergence is:  tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "kl_grad_vector is:  tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', grad_fn=<CatBackward0>)\n",
      "hp at 4 is:  tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\n",
      "x at 4 is:  tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\n",
      "kl_divergence is:  tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "kl_grad_vector is:  tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', grad_fn=<CatBackward0>)\n",
      "hp at 5 is:  tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\n",
      "x at 5 is:  tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\n",
      "kl_divergence is:  tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "kl_grad_vector is:  tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', grad_fn=<CatBackward0>)\n",
      "hp at 6 is:  tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\n",
      "x at 6 is:  tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\n",
      "kl_divergence is:  tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "kl_grad_vector is:  tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', grad_fn=<CatBackward0>)\n",
      "hp at 7 is:  tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\n",
      "x at 7 is:  tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\n",
      "kl_divergence is:  tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "kl_grad_vector is:  tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', grad_fn=<CatBackward0>)\n",
      "hp at 8 is:  tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\n",
      "x at 8 is:  tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\n",
      "kl_divergence is:  tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "kl_grad_vector is:  tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', grad_fn=<CatBackward0>)\n",
      "hp at 9 is:  tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\n",
      "x at 9 is:  tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\n",
      "x finally: :  tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\n",
      "kl_divergence is:  tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "kl_grad_vector is:  tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', grad_fn=<CatBackward0>)\n",
      "coef is:  1.0\n",
      "descent is:  tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "y is:  tensor([[0.5133, 0.4867],\n",
      "        [0.5351, 0.4649],\n",
      "        [0.5150, 0.4850],\n",
      "        [0.5362, 0.4638],\n",
      "        [0.5170, 0.4830],\n",
      "        [0.4942, 0.5058],\n",
      "        [0.4755, 0.5245],\n",
      "        [0.4543, 0.5457],\n",
      "        [0.4286, 0.5714],\n",
      "        [0.4570, 0.5430],\n",
      "        [0.4784, 0.5216],\n",
      "        [0.4955, 0.5045],\n",
      "        [0.4783, 0.5217],\n",
      "        [0.4580, 0.5420],\n",
      "        [0.4332, 0.5668],\n",
      "        [0.4585, 0.5415],\n",
      "        [0.4774, 0.5226],\n",
      "        [0.4567, 0.5433],\n",
      "        [0.4756, 0.5244],\n",
      "        [0.4862, 0.5138],\n",
      "        [0.5037, 0.4963],\n",
      "        [0.4830, 0.5170],\n",
      "        [0.4688, 0.5312],\n",
      "        [0.4445, 0.5555],\n",
      "        [0.4658, 0.5342],\n",
      "        [0.4405, 0.5595],\n",
      "        [0.4181, 0.5819]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "\u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mtrain_test_util\u001b[0m: - \u001b[31m\u001b[1merror occured: Expected parameter probs (Tensor of shape (27, 2)) of distribution Categorical(probs: torch.Size([27, 2])) to satisfy the constraint Simplex(), but found invalid values:\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]], device='cuda:0', grad_fn=<DivBackward0>)\u001b[0m; \u001b[32m2024-07-09 01:43:21\u001b[0m \u001b[36mstart_train\u001b[0m:\u001b[36m74\u001b[0m\n",
      "\u001b[33m\u001b[1mTraceback (most recent call last):\u001b[0m\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "    │   └ <bound method Application.launch_instance of <class 'ipykernel.kernelapp.IPKernelApp'>>\n",
      "    └ <module 'ipykernel.kernelapp' from 'f:\\\\conda\\\\envs\\\\quant\\\\Lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "    │   └ <function IPKernelApp.start at 0x0000029BE27B6A20>\n",
      "    └ <ipykernel.kernelapp.IPKernelApp object at 0x0000029BDD532B10>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "    │    │       └ <function BaseAsyncIOLoop.start at 0x0000029BE27B7B00>\n",
      "    │    └ <tornado.platform.asyncio.AsyncIOMainLoop object at 0x0000029BE27D5890>\n",
      "    └ <ipykernel.kernelapp.IPKernelApp object at 0x0000029BDD532B10>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "    │    │            └ <function BaseEventLoop.run_forever at 0x0000029BDF2F9E40>\n",
      "    │    └ <_WindowsSelectorEventLoop running=True closed=False debug=False>\n",
      "    └ <tornado.platform.asyncio.AsyncIOMainLoop object at 0x0000029BE27D5890>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "    │    └ <function BaseEventLoop._run_once at 0x0000029BDF2FBC40>\n",
      "    └ <_WindowsSelectorEventLoop running=True closed=False debug=False>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "    │      └ <function Handle._run at 0x0000029BDF2889A0>\n",
      "    └ <Handle Task.task_wakeup(<Future finis...9B)>, ...],))>)>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "    │    │            │    │           │    └ <member '_args' of 'Handle' objects>\n",
      "    │    │            │    │           └ <Handle Task.task_wakeup(<Future finis...9B)>, ...],))>)>\n",
      "    │    │            │    └ <member '_callback' of 'Handle' objects>\n",
      "    │    │            └ <Handle Task.task_wakeup(<Future finis...9B)>, ...],))>)>\n",
      "    │    └ <member '_context' of 'Handle' objects>\n",
      "    └ <Handle Task.task_wakeup(<Future finis...9B)>, ...],))>)>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "          │    └ <function Kernel.process_one at 0x0000029BE13068E0>\n",
      "          └ <ipykernel.ipkernel.IPythonKernel object at 0x0000029BE27DF550>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "          │         └ ([<zmq.Frame(b'1ba77f4a-045'...36B)>, <zmq.Frame(b'<IDS|MSG>')>, <zmq.Frame(b'e7d2310b1e4e'...64B)>, <zmq.Frame(b'{\"date\":\"20...\n",
      "          └ <bound method Kernel.dispatch_shell of <ipykernel.ipkernel.IPythonKernel object at 0x0000029BE27DF550>>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "          └ <coroutine object IPythonKernel.execute_request at 0x0000029BA183B740>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 359, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "                                  │       │      └ {'header': {'date': datetime.datetime(2024, 7, 8, 17, 43, 20, 896000, tzinfo=tzutc()), 'msg_id': 'db7c90ed-78ad-41d7-ab05-411...\n",
      "                                  │       └ [b'1ba77f4a-0457-4fca-adbb-0327ece6511f']\n",
      "                                  └ <zmq.eventloop.zmqstream.ZMQStream object at 0x0000029BE27D6550>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "                          └ <coroutine object IPythonKernel.do_execute at 0x0000029BE279C810>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 446, in do_execute\n",
      "    res = shell.run_cell(\n",
      "          │     └ <function ZMQInteractiveShell.run_cell at 0x0000029BE2797BA0>\n",
      "          └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x0000029BE27FCB50>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "                             │       └ {'store_history': True, 'silent': False, 'cell_id': 'vscode-notebook-cell:/f%3A/ws/rf_learning/policy_based_learning.ipynb#X5...\n",
      "                             └ ('start_train(\\n    StandarTrainProcess(\\n        trainer=policy_func_trainer,\\n        model=policy_func,\\n        train_epo...\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "             │    └ <function InteractiveShell._run_cell at 0x0000029BE094F100>\n",
      "             └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x0000029BE27FCB50>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "             │      └ <coroutine object InteractiveShell.run_cell_async at 0x0000029BE25EADC0>\n",
      "             └ <function _pseudo_sync_runner at 0x0000029BE09462A0>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "    │    └ <method 'send' of 'coroutine' objects>\n",
      "    └ <coroutine object InteractiveShell.run_cell_async at 0x0000029BE25EADC0>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "                       │    │             │        │     └ 'C:\\\\Users\\\\29000\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_17112\\\\284164877.py'\n",
      "                       │    │             │        └ [<ast.Expr object at 0x0000029B8926BA30>]\n",
      "                       │    │             └ <ast.Module object at 0x0000029BE285C1C0>\n",
      "                       │    └ <function InteractiveShell.run_ast_nodes at 0x0000029BE094F420>\n",
      "                       └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x0000029BE27FCB50>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "             │    │        │     │              └ False\n",
      "             │    │        │     └ <ExecutionResult object at 29be2852d10, execution_count=3 error_before_exec=None error_in_exec=None info=<ExecutionInfo objec...\n",
      "             │    │        └ <code object <module> at 0x0000029BE27FB2D0, file \"C:\\Users\\29000\\AppData\\Local\\Temp\\ipykernel_17112\\284164877.py\", line 1>\n",
      "             │    └ <function InteractiveShell.run_code at 0x0000029BE094F4C0>\n",
      "             └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x0000029BE27FCB50>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "         │         │    │               │    └ {'__name__': '__main__', '__doc__': 'Automatically created module for IPython interactive environment', '__package__': None, ...\n",
      "         │         │    │               └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x0000029BE27FCB50>\n",
      "         │         │    └ <property object at 0x0000029BE093F4C0>\n",
      "         │         └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x0000029BE27FCB50>\n",
      "         └ <code object <module> at 0x0000029BE27FB2D0, file \"C:\\Users\\29000\\AppData\\Local\\Temp\\ipykernel_17112\\284164877.py\", line 1>\n",
      "\n",
      "  File \"\u001b[32mC:\\Users\\29000\\AppData\\Local\\Temp\\ipykernel_17112\\\u001b[0m\u001b[32m\u001b[1m284164877.py\u001b[0m\", line \u001b[33m1\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    \u001b[1mstart_train\u001b[0m\u001b[1m(\u001b[0m\n",
      "    \u001b[36m└ \u001b[0m\u001b[36m\u001b[1m<function start_train at 0x0000029B89267060>\u001b[0m\n",
      "\n",
      "> File \"\u001b[32mf:\\ws\\rf_learning\\\u001b[0m\u001b[32m\u001b[1mtrain_test_util.py\u001b[0m\", line \u001b[33m69\u001b[0m, in \u001b[35mstart_train\u001b[0m\n",
      "    \u001b[1mtrain_process\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mtrainer\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mtrain\u001b[0m\u001b[1m(\u001b[0m\u001b[1mtrain_epoch\u001b[0m\u001b[35m\u001b[1m=\u001b[0m\u001b[1mtrain_process\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mtrain_epoch\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m│             │       │                 │             └ \u001b[0m\u001b[36m\u001b[1m3000\u001b[0m\n",
      "    \u001b[36m│             │       │                 └ \u001b[0m\u001b[36m\u001b[1mStandarTrainProcess(trainer=<policy_based.PolicyNetTrainerWithTRPO object at 0x0000029B87ACA8D0>, model=PolicyNetFunc(\u001b[0m\n",
      "    \u001b[36m│             │       │                   \u001b[0m\u001b[36m\u001b[1m  (_fc...\u001b[0m\n",
      "    \u001b[36m│             │       └ \u001b[0m\u001b[36m\u001b[1m<function PolicyNetTrainer.train at 0x0000029B89265DA0>\u001b[0m\n",
      "    \u001b[36m│             └ \u001b[0m\u001b[36m\u001b[1m<policy_based.PolicyNetTrainerWithTRPO object at 0x0000029B87ACA8D0>\u001b[0m\n",
      "    \u001b[36m└ \u001b[0m\u001b[36m\u001b[1mStandarTrainProcess(trainer=<policy_based.PolicyNetTrainerWithTRPO object at 0x0000029B87ACA8D0>, model=PolicyNetFunc(\u001b[0m\n",
      "    \u001b[36m  \u001b[0m\u001b[36m\u001b[1m  (_fc...\u001b[0m\n",
      "\n",
      "  File \"\u001b[32mf:\\ws\\rf_learning\\\u001b[0m\u001b[32m\u001b[1mpolicy_based.py\u001b[0m\", line \u001b[33m162\u001b[0m, in \u001b[35mtrain\u001b[0m\n",
      "    \u001b[1mself\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mupdate\u001b[0m\u001b[1m(\u001b[0m\u001b[1mtrajectory_record_list\u001b[0m\u001b[1m,\u001b[0m \u001b[1mwriter\u001b[0m\u001b[1m,\u001b[0m \u001b[1mepoch\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m│    │      │                       │       └ \u001b[0m\u001b[36m\u001b[1m0\u001b[0m\n",
      "    \u001b[36m│    │      │                       └ \u001b[0m\u001b[36m\u001b[1m<tensorboardX.writer.SummaryWriter object at 0x0000029B9ED6B510>\u001b[0m\n",
      "    \u001b[36m│    │      └ \u001b[0m\u001b[36m\u001b[1m[(array([-0.03250131,  0.01840519,  0.04241987,  0.01009412], dtype=float32), 0, 1.0, array([-0.0321332 , -0.17729864,  0.042...\u001b[0m\n",
      "    \u001b[36m│    └ \u001b[0m\u001b[36m\u001b[1m<function PolicyNetTrainerWithTRPO.update at 0x0000029B89266700>\u001b[0m\n",
      "    \u001b[36m└ \u001b[0m\u001b[36m\u001b[1m<policy_based.PolicyNetTrainerWithTRPO object at 0x0000029B87ACA8D0>\u001b[0m\n",
      "\n",
      "  File \"\u001b[32mf:\\ws\\rf_learning\\\u001b[0m\u001b[32m\u001b[1mpolicy_based.py\u001b[0m\", line \u001b[33m478\u001b[0m, in \u001b[35mupdate\u001b[0m\n",
      "    \u001b[1mself\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1m_update_with_trpo\u001b[0m\u001b[1m(\u001b[0m\u001b[1mtrajectory_record_list\u001b[0m\u001b[1m,\u001b[0m \u001b[1mwriter\u001b[0m\u001b[1m,\u001b[0m \u001b[1mepoch\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m│    │                 │                       │       └ \u001b[0m\u001b[36m\u001b[1m0\u001b[0m\n",
      "    \u001b[36m│    │                 │                       └ \u001b[0m\u001b[36m\u001b[1m<tensorboardX.writer.SummaryWriter object at 0x0000029B9ED6B510>\u001b[0m\n",
      "    \u001b[36m│    │                 └ \u001b[0m\u001b[36m\u001b[1m[(array([-0.03250131,  0.01840519,  0.04241987,  0.01009412], dtype=float32), 0, 1.0, array([-0.0321332 , -0.17729864,  0.042...\u001b[0m\n",
      "    \u001b[36m│    └ \u001b[0m\u001b[36m\u001b[1m<function PolicyNetTrainerWithTRPO._update_with_trpo at 0x0000029B89266A20>\u001b[0m\n",
      "    \u001b[36m└ \u001b[0m\u001b[36m\u001b[1m<policy_based.PolicyNetTrainerWithTRPO object at 0x0000029B87ACA8D0>\u001b[0m\n",
      "\n",
      "  File \"\u001b[32mf:\\ws\\rf_learning\\\u001b[0m\u001b[32m\u001b[1mpolicy_based.py\u001b[0m\", line \u001b[33m612\u001b[0m, in \u001b[35m_update_with_trpo\u001b[0m\n",
      "    \u001b[1mnew_para\u001b[0m \u001b[35m\u001b[1m=\u001b[0m \u001b[1mself\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1m_linear_search\u001b[0m\u001b[1m(\u001b[0m\n",
      "    \u001b[36m           │    └ \u001b[0m\u001b[36m\u001b[1m<function PolicyNetTrainerWithTRPO._linear_search at 0x0000029B89266980>\u001b[0m\n",
      "    \u001b[36m           └ \u001b[0m\u001b[36m\u001b[1m<policy_based.PolicyNetTrainerWithTRPO object at 0x0000029B87ACA8D0>\u001b[0m\n",
      "\n",
      "  File \"\u001b[32mf:\\ws\\rf_learning\\\u001b[0m\u001b[32m\u001b[1mpolicy_based.py\u001b[0m\", line \u001b[33m550\u001b[0m, in \u001b[35m_linear_search\u001b[0m\n",
      "    \u001b[1mnew_action_dist\u001b[0m \u001b[35m\u001b[1m=\u001b[0m \u001b[1mtorch\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mdistributions\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mCategorical\u001b[0m\u001b[1m(\u001b[0m\u001b[1mx\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m                  │     │             │           └ \u001b[0m\u001b[36m\u001b[1mtensor([[nan, nan],\u001b[0m\n",
      "    \u001b[36m                  │     │             │             \u001b[0m\u001b[36m\u001b[1m        [nan, nan],\u001b[0m\n",
      "    \u001b[36m                  │     │             │             \u001b[0m\u001b[36m\u001b[1m        [nan, nan],\u001b[0m\n",
      "    \u001b[36m                  │     │             │             \u001b[0m\u001b[36m\u001b[1m        [nan, nan],\u001b[0m\n",
      "    \u001b[36m                  │     │             │             \u001b[0m\u001b[36m\u001b[1m        [nan, nan],\u001b[0m\n",
      "    \u001b[36m                  │     │             │             \u001b[0m\u001b[36m\u001b[1m        [nan, nan],\u001b[0m\n",
      "    \u001b[36m                  │     │             │             \u001b[0m\u001b[36m\u001b[1m     ...\u001b[0m\n",
      "    \u001b[36m                  │     │             └ \u001b[0m\u001b[36m\u001b[1m<class 'torch.distributions.categorical.Categorical'>\u001b[0m\n",
      "    \u001b[36m                  │     └ \u001b[0m\u001b[36m\u001b[1m<module 'torch.distributions' from 'f:\\\\conda\\\\envs\\\\quant\\\\Lib\\\\site-packages\\\\torch\\\\distributions\\\\__init__.py'>\u001b[0m\n",
      "    \u001b[36m                  └ \u001b[0m\u001b[36m\u001b[1m<module 'torch' from 'f:\\\\conda\\\\envs\\\\quant\\\\Lib\\\\site-packages\\\\torch\\\\__init__.py'>\u001b[0m\n",
      "\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\torch\\distributions\\categorical.py\", line 70, in __init__\n",
      "    super().__init__(batch_shape, validate_args=validate_args)\n",
      "                     │                          └ None\n",
      "                     └ torch.Size([27])\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\torch\\distributions\\distribution.py\", line 69, in __init__\n",
      "    raise ValueError(\n",
      "\n",
      "\u001b[31m\u001b[1mValueError\u001b[0m:\u001b[1m Expected parameter probs (Tensor of shape (27, 2)) of distribution Categorical(probs: torch.Size([27, 2])) to satisfy the constraint Simplex(), but found invalid values:\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]], device='cuda:0', grad_fn=<DivBackward0>)\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtrain_test_util\u001b[0m: - \u001b[1msaving model to: run\\model\\cartpoleV1\\AC\\AC.pth,\u001b[0m; \u001b[32m2024-07-09 01:43:21\u001b[0m \u001b[36mstart_train\u001b[0m:\u001b[36m76\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_train(\n",
    "    StandarTrainProcess(\n",
    "        trainer=policy_func_trainer,\n",
    "        model=policy_func,\n",
    "        train_epoch=TRAIN_EPOCH,\n",
    "        log_path=LOG_PATH,\n",
    "        model_path=MODEL_PATH\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
