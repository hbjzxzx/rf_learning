{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "from policy_based import PolicyNetFunc, PolicyNetTrainer, PolicyNetTester\n",
    "from deep_q import Discrete1ContinuousAction\n",
    "from env import Env\n",
    "from utils import clear_target_path, show_gif_on_jupyternb, to_gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab1 Policy-Base Train\n",
    "\n",
    "使用CarPole-V1 环境，测试Policy-Based REINFORCE 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GYM_ENV_NAME = 'CartPole-v1'\n",
    "_train_gym_env = gym.make(GYM_ENV_NAME)\n",
    "env = Env(_train_gym_env)\n",
    "\n",
    "# 打印查看环境的动作空间和状态空间 \n",
    "action_nums, state_space = _train_gym_env.action_space.n, _train_gym_env.observation_space\n",
    "print(f'action num: {action_nums}, space: {state_space}')\n",
    "\n",
    "TRAIN_EPOCH = 1000\n",
    "HIDDEN_DIM = 256\n",
    "LEARNING_RATE = 2e-3\n",
    "GAMMA = 0.99\n",
    "\n",
    "\n",
    "LOG_PATH = Path('./run/logs/cartpoleV1/REINFORCE')\n",
    "MODEL_PATH = Path('./run/model/cartpoleV1/REINFORCE.pth')\n",
    "TEST_OUTPUT_PATH = Path('./run/test_result/cartpoleV1_REINFORCE')\n",
    "\n",
    "_USE_CUDA = True and torch.cuda.is_available()\n",
    "# _USE_CUDA = False and torch.cuda.is_available()\n",
    "\n",
    "policy_func = PolicyNetFunc(state_space.shape[0], \n",
    "                   action_nums, \n",
    "                   hidden_dim=HIDDEN_DIM, \n",
    "                   device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "\n",
    "policy_func_trainer = PolicyNetTrainer(policy_func=policy_func,\n",
    "                                  env=env,\n",
    "                                  learning_rate=LEARNING_RATE,\n",
    "                                  gamma=GAMMA,\n",
    "                                  logger_folder=LOG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_target_path(LOG_PATH)\n",
    "clear_target_path(MODEL_PATH)\n",
    "print(f'start training, now datetime: {datetime.datetime.now()}')\n",
    "policy_func_trainer.train(train_epoch=TRAIN_EPOCH)\n",
    "print(f'end training, saving model to: {MODEL_PATH}, now datetime: {datetime.datetime.now()}')\n",
    "\n",
    "policy_func.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_policy_func = PolicyNetFunc.from_file(MODEL_PATH)\n",
    "_render_env = Env(gym.make(GYM_ENV_NAME, render_mode='rgb_array_list'))\n",
    "policy_func_tester = PolicyNetTester(\n",
    "    policy_fun=test_policy_func.to('cpu'),\n",
    "    env=_render_env\n",
    ")\n",
    "RESULT_GIF = TEST_OUTPUT_PATH / 'result.gif'\n",
    "clear_target_path(RESULT_GIF)\n",
    "policy_func_tester.test(1000)\n",
    "to_gif(_render_env._gym_env, RESULT_GIF, 1/30)\n",
    "\n",
    "show_gif_on_jupyternb(RESULT_GIF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Pendulum-v1 环境，测试Policy-Based REINFORCE 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GYM_ENV_NAME = 'Pendulum-v1'\n",
    "RESULT_DIR_NAME = 'pendulumV1'\n",
    "\n",
    "_train_gym_env = gym.make(GYM_ENV_NAME)\n",
    "env = Env(_train_gym_env)\n",
    "\n",
    "LOG_PATH = Path(f'./run/logs/{RESULT_DIR_NAME}/policy_based')\n",
    "MODEL_PATH = Path(f'./run/model/{RESULT_DIR_NAME}/policy_model.pth')\n",
    "TEST_OUTPUT_PATH = Path(f'./run/test_result/{RESULT_DIR_NAME}_policy')\n",
    "\n",
    "# 打印查看环境的动作空间和状态空间 \n",
    "action_space, state_space = _train_gym_env.action_space, _train_gym_env.observation_space\n",
    "print(f'action: {action_space}, space: {state_space}')\n",
    "\n",
    "# 动作空间离散化程度（用11个区间来替代连续动作空间）\n",
    "BINS = 11\n",
    "\n",
    "TRAIN_EPOCH = 1000\n",
    "HIDDEN_DIM = 512\n",
    "LEARNING_RATE = 1e-3\n",
    "GAMMA = 0.99\n",
    "\n",
    "_USE_CUDA = True and torch.cuda.is_available()\n",
    "# _USE_CUDA = False and torch.cuda.is_available()\n",
    "\n",
    "policy_func = PolicyNetFunc(state_space.shape[0], \n",
    "                   action_nums=BINS, \n",
    "                   hidden_dim=HIDDEN_DIM, \n",
    "                   device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "\n",
    "policy_func_trainer = PolicyNetTrainer(policy_func=policy_func,\n",
    "                                  env=env,\n",
    "                                  learning_rate=LEARNING_RATE,\n",
    "                                  gamma=GAMMA,\n",
    "                                  logger_folder=LOG_PATH,\n",
    "                                  action_converter=Discrete1ContinuousAction(action_space.low, action_space.high, BINS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_target_path(LOG_PATH)\n",
    "clear_target_path(MODEL_PATH)\n",
    "print(f'start training, now datetime: {datetime.datetime.now()}')\n",
    "policy_func_trainer.train(train_epoch=TRAIN_EPOCH)\n",
    "print(f'end training, saving model to: {MODEL_PATH}, now datetime: {datetime.datetime.now()}')\n",
    "\n",
    "policy_func.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_policy_func = PolicyNetFunc.from_file(MODEL_PATH)\n",
    "_render_env = Env(gym.make(GYM_ENV_NAME, render_mode='rgb_array_list'))\n",
    "policy_func_tester = PolicyNetTester(\n",
    "    policy_fun=test_policy_func.to('cpu'),\n",
    "    env=_render_env,\n",
    "    action_converter=Discrete1ContinuousAction(action_space.low, action_space.high, BINS)\n",
    ")\n",
    "RESULT_GIF = TEST_OUTPUT_PATH / 'result.gif'\n",
    "clear_target_path(RESULT_GIF)\n",
    "policy_func_tester.test(1000)\n",
    "to_gif(_render_env._gym_env, RESULT_GIF, 1/30)\n",
    "\n",
    "show_gif_on_jupyternb(RESULT_GIF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Pendulum-v1 环境，测试Policy-Based AC 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: Box(-2.0, 2.0, (1,), float32), space: Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)\n"
     ]
    }
   ],
   "source": [
    "from policy_based import PolicyValueNetTrainer, ValueNetFunc\n",
    "\n",
    "GYM_ENV_NAME = 'Pendulum-v1'\n",
    "RESULT_DIR_NAME = 'pendulumV1'\n",
    "\n",
    "_train_gym_env = gym.make(GYM_ENV_NAME)\n",
    "env = Env(_train_gym_env)\n",
    "\n",
    "LOG_PATH = Path(f'./run/logs/{RESULT_DIR_NAME}/AC')\n",
    "MODEL_PATH = Path(f'./run/model/{RESULT_DIR_NAME}/AC.pth')\n",
    "TEST_OUTPUT_PATH = Path(f'./run/test_result/{RESULT_DIR_NAME}_AC')\n",
    "\n",
    "# 打印查看环境的动作空间和状态空间 \n",
    "action_space, state_space = _train_gym_env.action_space, _train_gym_env.observation_space\n",
    "print(f'action: {action_space}, space: {state_space}')\n",
    "\n",
    "# 动作空间离散化程度（用11个区间来替代连续动作空间）\n",
    "BINS = 11\n",
    "\n",
    "TRAIN_EPOCH = 1000\n",
    "HIDDEN_DIM = 512\n",
    "LEARNING_RATE = 1e-3\n",
    "GAMMA = 0.99\n",
    "\n",
    "_USE_CUDA = True and torch.cuda.is_available()\n",
    "# _USE_CUDA = False and torch.cuda.is_available()\n",
    "\n",
    "policy_func = PolicyNetFunc(state_space.shape[0], \n",
    "                   action_nums=BINS, \n",
    "                   hidden_dim=HIDDEN_DIM, \n",
    "                   device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "value_func = ValueNetFunc(state_space.shape[0],\n",
    "                          action_nums=BINS,\n",
    "                          hidden_dim=HIDDEN_DIM,\n",
    "                          device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "\n",
    "policy_func_trainer = PolicyValueNetTrainer(\n",
    "                                  policy_func=policy_func,\n",
    "                                  value_func=value_func,\n",
    "                                  env=env,\n",
    "                                  learning_rate=LEARNING_RATE,\n",
    "                                  gamma=GAMMA,\n",
    "                                  logger_folder=LOG_PATH,\n",
    "                                  action_converter=Discrete1ContinuousAction(action_space.low, action_space.high, BINS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training, now datetime: 2024-06-24 14:38:13.017305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:15<00:00, 13.25it/s, reward=-1652.73, step=200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end training, saving model to: run/model/pendulumV1/AC.pth, now datetime: 2024-06-24 14:39:28.474330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clear_target_path(LOG_PATH)\n",
    "clear_target_path(MODEL_PATH)\n",
    "print(f'start training, now datetime: {datetime.datetime.now()}')\n",
    "policy_func_trainer.train(train_epoch=TRAIN_EPOCH)\n",
    "print(f'end training, saving model to: {MODEL_PATH}, now datetime: {datetime.datetime.now()}')\n",
    "\n",
    "policy_func.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
