{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "from policy_based import PolicyNetFunc, PolicyNetTrainer, PolicyNetTester, PolicyNetTrainerWithBase, ValueNetFunc\n",
    "from deep_q import Discrete1ContinuousAction\n",
    "from env import Env, get_action_discreter\n",
    "from utils import clear_target_path, show_gif_on_jupyternb, to_gif\n",
    "from train_test_util import start_test, start_train, StandarTestProcess, StandarTrainProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用CarPole-V1 环境，测试Policy-Based REINFORCE 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GYM_ENV_NAME = 'CartPole-v1'\n",
    "env = Env.from_env_name(GYM_ENV_NAME)\n",
    "RESULT_DIR_NAME='cartpoleV1'\n",
    "\n",
    "LOG_PATH = Path(f'./run/logs/{RESULT_DIR_NAME}/REINFORCE')\n",
    "MODEL_PATH = Path(f'./run/model/{RESULT_DIR_NAME}/REINFORCE.pth')\n",
    "TEST_OUTPUT_PATH = Path(f'./run/test_result/{RESULT_DIR_NAME}/REINFORCE')\n",
    "\n",
    "# 打印查看环境的动作空间和状态空间 \n",
    "env.print_state_action_dims()\n",
    "\n",
    "\n",
    "TRAIN_EPOCH = 1000\n",
    "HIDDEN_DIM = 256\n",
    "LEARNING_RATE = 2e-3\n",
    "GAMMA = 0.99\n",
    "\n",
    "_USE_CUDA = True and torch.cuda.is_available()\n",
    "# _USE_CUDA = False and torch.cuda.is_available()\n",
    "\n",
    "policy_func = PolicyNetFunc(env.get_state_dim()[0], \n",
    "                   env.get_action_dim()[0], \n",
    "                   hidden_dim=HIDDEN_DIM, \n",
    "                   device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "\n",
    "policy_func_trainer = PolicyNetTrainer(policy_func=policy_func,\n",
    "                                  env=env,\n",
    "                                  learning_rate=LEARNING_RATE,\n",
    "                                  gamma=GAMMA,\n",
    "                                  logger_folder=LOG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_train(StandarTrainProcess(\n",
    "    trainer=policy_func_trainer,\n",
    "    model=policy_func,\n",
    "    train_epoch=TRAIN_EPOCH,\n",
    "    log_path=LOG_PATH,\n",
    "    model_path=MODEL_PATH\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_policy_func = PolicyNetFunc.from_file(MODEL_PATH)\n",
    "_render_env = Env(gym.make(GYM_ENV_NAME, render_mode='rgb_array_list'))\n",
    "\n",
    "policy_func_tester = PolicyNetTester(\n",
    "    policy_fun=test_policy_func,\n",
    "    env=_render_env\n",
    ")\n",
    "\n",
    "\n",
    "start_test(\n",
    "    StandarTestProcess(\n",
    "        model=test_policy_func,\n",
    "        tester=policy_func_tester,\n",
    "        env=_render_env,\n",
    "        test_output_path=TEST_OUTPUT_PATH,\n",
    "        test_epoch=1000,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用CarPole-V1 环境，测试Policy-Based + 基线的 REINFORCE 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GYM_ENV_NAME = 'CartPole-v1'\n",
    "RESULT_DIR_NAME='cartpoleV1'\n",
    "env = Env.from_env_name(GYM_ENV_NAME)\n",
    "\n",
    "LOG_PATH = Path(f'./run/logs/{RESULT_DIR_NAME}/REINFORCE_With_BASE')\n",
    "MODEL_PATH = Path(f'./run/model/{RESULT_DIR_NAME}/REINFORCE_With_BASE.pth')\n",
    "TEST_OUTPUT_PATH = Path(f'./run/test_result/{RESULT_DIR_NAME}/REINFORCE_With_BASE')\n",
    "\n",
    "# 打印查看环境的动作空间和状态空间 \n",
    "env.print_state_action_dims()\n",
    "\n",
    "\n",
    "TRAIN_EPOCH = 1000\n",
    "HIDDEN_DIM = 256\n",
    "LEARNING_RATE = 2e-3\n",
    "VALUE_LEARNING_RATE = 1e-3\n",
    "GAMMA = 0.99\n",
    "\n",
    "_USE_CUDA = True and torch.cuda.is_available()\n",
    "# _USE_CUDA = False and torch.cuda.is_available()\n",
    "\n",
    "value_func = ValueNetFunc(\n",
    "                env.get_state_dim()[0], \n",
    "                hidden_dim=HIDDEN_DIM, \n",
    "                device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "policy_func = PolicyNetFunc(env.get_state_dim()[0], \n",
    "                   env.get_action_dim()[0], \n",
    "                   hidden_dim=HIDDEN_DIM, \n",
    "                   device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "\n",
    "policy_func_trainer = PolicyNetTrainerWithBase(\n",
    "                                  policy_func=policy_func,\n",
    "                                  value_func=value_func,\n",
    "                                  value_learning_rate=LEARNING_RATE,\n",
    "                                  env=env,\n",
    "                                  learning_rate=VALUE_LEARNING_RATE,\n",
    "                                  gamma=GAMMA,\n",
    "                                  logger_folder=LOG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_train(StandarTrainProcess(\n",
    "    trainer=policy_func_trainer,\n",
    "    model=policy_func,\n",
    "    train_epoch=TRAIN_EPOCH,\n",
    "    log_path=LOG_PATH,\n",
    "    model_path=MODEL_PATH\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_policy_func = PolicyNetFunc.from_file(MODEL_PATH)\n",
    "_render_env = Env(gym.make(GYM_ENV_NAME, render_mode='rgb_array_list'))\n",
    "\n",
    "policy_func_tester = PolicyNetTester(\n",
    "    policy_fun=test_policy_func,\n",
    "    env=_render_env\n",
    ")\n",
    "\n",
    "\n",
    "start_test(\n",
    "    StandarTestProcess(\n",
    "        model=test_policy_func,\n",
    "        tester=policy_func_tester,\n",
    "        env=_render_env,\n",
    "        test_output_path=TEST_OUTPUT_PATH,\n",
    "        test_epoch=1000,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Pendulum-v1 环境，测试Policy-Based REINFORCE 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GYM_ENV_NAME = 'Pendulum-v1'\n",
    "RESULT_DIR_NAME = 'pendulumV1'\n",
    "\n",
    "env = Env.from_env_name(GYM_ENV_NAME)\n",
    "\n",
    "LOG_PATH = Path(f'./run/logs/{RESULT_DIR_NAME}/policy_based')\n",
    "MODEL_PATH = Path(f'./run/model/{RESULT_DIR_NAME}/policy_model.pth')\n",
    "TEST_OUTPUT_PATH = Path(f'./run/test_result/{RESULT_DIR_NAME}_policy')\n",
    "\n",
    "# 打印查看环境的动作空间和状态空间 \n",
    "env.print_state_action_dims()\n",
    "\n",
    "# 动作空间离散化程度（用11个区间来替代连续动作空间）\n",
    "BINS = 11\n",
    "\n",
    "TRAIN_EPOCH = 1000\n",
    "HIDDEN_DIM = 512\n",
    "LEARNING_RATE = 1e-3\n",
    "GAMMA = 0.99\n",
    "\n",
    "_USE_CUDA = True and torch.cuda.is_available()\n",
    "# _USE_CUDA = False and torch.cuda.is_available()\n",
    "\n",
    "policy_func = PolicyNetFunc(env.get_state_dim()[0], \n",
    "                   action_nums=BINS, \n",
    "                   hidden_dim=HIDDEN_DIM, \n",
    "                   device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "\n",
    "policy_func_trainer = PolicyNetTrainer(policy_func=policy_func,\n",
    "                                  env=env,\n",
    "                                  learning_rate=LEARNING_RATE,\n",
    "                                  gamma=GAMMA,\n",
    "                                  logger_folder=LOG_PATH,\n",
    "                                  action_converter=get_action_discreter(env, BINS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_train(StandarTrainProcess(\n",
    "    trainer=policy_func_trainer,\n",
    "    model=policy_func,\n",
    "    train_epoch=TRAIN_EPOCH,\n",
    "    log_path=LOG_PATH,\n",
    "    model_path=MODEL_PATH\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_policy_func = PolicyNetFunc.from_file(MODEL_PATH)\n",
    "_render_env = Env(gym.make(GYM_ENV_NAME, render_mode='rgb_array_list'))\n",
    "\n",
    "policy_func_tester = PolicyNetTester(\n",
    "    policy_fun=test_policy_func,\n",
    "    env=_render_env,\n",
    "    action_converter=get_action_discreter(env, BINS)\n",
    ")\n",
    "\n",
    "\n",
    "start_test(\n",
    "    StandarTestProcess(\n",
    "        model=test_policy_func,\n",
    "        tester=policy_func_tester,\n",
    "        env=_render_env,\n",
    "        test_output_path=TEST_OUTPUT_PATH,\n",
    "        test_epoch=1000,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Pendulum-v1 环境，测试Policy-Based + 基线的 REINFORCE 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GYM_ENV_NAME = 'Pendulum-v1'\n",
    "RESULT_DIR_NAME = 'pendulumV1'\n",
    "\n",
    "env = Env.from_env_name(GYM_ENV_NAME)\n",
    "\n",
    "LOG_PATH = Path(f'./run/logs/{RESULT_DIR_NAME}/REINFORCE_With_BASE')\n",
    "MODEL_PATH = Path(f'./run/model/{RESULT_DIR_NAME}/REINFORCE_With_BASE.pth')\n",
    "TEST_OUTPUT_PATH = Path(f'./run/test_result/{RESULT_DIR_NAME}/REINFORCE_With_BASE')\n",
    "\n",
    "# 打印查看环境的动作空间和状态空间 \n",
    "env.print_state_action_dims()\n",
    "\n",
    "# 动作空间离散化程度（用11个区间来替代连续动作空间）\n",
    "BINS = 11\n",
    "\n",
    "TRAIN_EPOCH = 3000\n",
    "HIDDEN_DIM = 512\n",
    "LEARNING_RATE = 5e-4\n",
    "VALUE_LEARNING_RATE = 5e-3\n",
    "GAMMA = 0.999\n",
    "\n",
    "_USE_CUDA = True and torch.cuda.is_available()\n",
    "# _USE_CUDA = False and torch.cuda.is_available()\n",
    "\n",
    "value_func = ValueNetFunc(\n",
    "                env.get_state_dim()[0], \n",
    "                hidden_dim=HIDDEN_DIM, \n",
    "                device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "policy_func = PolicyNetFunc(env.get_state_dim()[0], \n",
    "                   action_nums=BINS, \n",
    "                   hidden_dim=HIDDEN_DIM, \n",
    "                   device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "\n",
    "policy_func_trainer = PolicyNetTrainerWithBase(\n",
    "                                  policy_func=policy_func,\n",
    "                                  value_func=value_func,\n",
    "                                  value_learning_rate=VALUE_LEARNING_RATE,\n",
    "                                  env=env,\n",
    "                                  learning_rate=LEARNING_RATE,\n",
    "                                  gamma=GAMMA,\n",
    "                                  logger_folder=LOG_PATH,\n",
    "                                  action_converter=get_action_discreter(env, BINS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_train(StandarTrainProcess(\n",
    "    trainer=policy_func_trainer,\n",
    "    model=policy_func,\n",
    "    train_epoch=TRAIN_EPOCH,\n",
    "    log_path=LOG_PATH,\n",
    "    model_path=MODEL_PATH\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_policy_func = PolicyNetFunc.from_file(MODEL_PATH)\n",
    "_render_env = Env(gym.make(GYM_ENV_NAME, render_mode='rgb_array_list'))\n",
    "\n",
    "policy_func_tester = PolicyNetTester(\n",
    "    policy_fun=test_policy_func,\n",
    "    env=_render_env,\n",
    "    action_converter=get_action_discreter(_render_env, BINS),\n",
    "    stochastic=True\n",
    ")\n",
    "\n",
    "\n",
    "start_test(\n",
    "    StandarTestProcess(\n",
    "        model=test_policy_func,\n",
    "        tester=policy_func_tester,\n",
    "        env=_render_env,\n",
    "        test_output_path=TEST_OUTPUT_PATH,\n",
    "        test_epoch=1000,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用CarPole-V1 环境，测试Policy-Based AC 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[36menv\u001b[0m: - \u001b[1maction: 2, space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\u001b[0m; \u001b[32m2024-06-27 07:42:13\u001b[0m \u001b[36mprint_state_action_dims\u001b[0m:\u001b[36m68\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from policy_based import PolicyValueNetTrainer, ActionStateValueNetFunc\n",
    "\n",
    "GYM_ENV_NAME = 'CartPole-v1'\n",
    "RESULT_DIR_NAME='cartpoleV1'\n",
    "env = Env.from_env_name(GYM_ENV_NAME)\n",
    "\n",
    "LOG_PATH = Path(f'./run/logs/{RESULT_DIR_NAME}/AC')\n",
    "MODEL_PATH = Path(f'./run/model/{RESULT_DIR_NAME}/AC/AC.pth')\n",
    "TEST_OUTPUT_PATH = Path(f'./run/test_result/{RESULT_DIR_NAME}/AC')\n",
    "\n",
    "# 打印查看环境的动作空间和状态空间 \n",
    "env.print_state_action_dims()\n",
    "\n",
    "TRAIN_EPOCH = 3000\n",
    "HIDDEN_DIM = 256\n",
    "LEARNING_RATE = 2e-3\n",
    "VLEARNING_RATE = 5e-3\n",
    "GAMMA = 0.99\n",
    "\n",
    "_USE_CUDA = True and torch.cuda.is_available()\n",
    "# _USE_CUDA = False and torch.cuda.is_available()\n",
    "\n",
    "policy_func = PolicyNetFunc(env.get_state_dim()[0], \n",
    "                   action_nums=env.get_action_dim()[0], \n",
    "                   hidden_dim=HIDDEN_DIM * 2, \n",
    "                   device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "value_func = ActionStateValueNetFunc(env.get_state_dim()[0],\n",
    "                          action_nums=env.get_action_dim()[0],\n",
    "                          hidden_dim=HIDDEN_DIM,\n",
    "                          device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "\n",
    "policy_func_trainer = PolicyValueNetTrainer(\n",
    "                                  policy_func=policy_func,\n",
    "                                  value_func=value_func,\n",
    "                                  vlearning_rate=VLEARNING_RATE,\n",
    "                                  env=env,\n",
    "                                  learning_rate=LEARNING_RATE,\n",
    "                                  gamma=GAMMA,\n",
    "                                  logger_folder=LOG_PATH\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtrain_test_util\u001b[0m: - \u001b[1mstart training, now datetime: 2024-06-27 07:42:18.131471\u001b[0m; \u001b[32m2024-06-27 07:42:18\u001b[0m \u001b[36mstart_train\u001b[0m:\u001b[36m49\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtrain_test_util\u001b[0m: - \u001b[1mFirst, clean log path: run\\logs\\cartpoleV1\\AC, and clean model path: run\\model\\cartpoleV1\\AC\\AC.pth\u001b[0m; \u001b[32m2024-06-27 07:42:18\u001b[0m \u001b[36mstart_train\u001b[0m:\u001b[36m50\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m: - \u001b[1mclear_target_path: run\\logs\\cartpoleV1\\AC dose not exist\u001b[0m; \u001b[32m2024-06-27 07:42:18\u001b[0m \u001b[36mclear_target_path\u001b[0m:\u001b[36m38\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtrain_test_util\u001b[0m: - \u001b[1mtrain started\u001b[0m; \u001b[32m2024-06-27 07:42:18\u001b[0m \u001b[36mstart_train\u001b[0m:\u001b[36m55\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [01:06<00:00, 45.35it/s, reward=62.00, step=62]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtrain_test_util\u001b[0m: - \u001b[1mend training, now datetime: 2024-06-27 07:43:24.283603\u001b[0m; \u001b[32m2024-06-27 07:43:24\u001b[0m \u001b[36mstart_train\u001b[0m:\u001b[36m57\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtrain_test_util\u001b[0m: - \u001b[1msaving model to: run\\model\\cartpoleV1\\AC\\AC.pth,\u001b[0m; \u001b[32m2024-06-27 07:43:24\u001b[0m \u001b[36mstart_train\u001b[0m:\u001b[36m63\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_train(\n",
    "    StandarTrainProcess(\n",
    "        trainer=policy_func_trainer,\n",
    "        model=policy_func,\n",
    "        train_epoch=TRAIN_EPOCH,\n",
    "        log_path=LOG_PATH,\n",
    "        model_path=MODEL_PATH\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtrain_test_util\u001b[0m: - \u001b[1mstart testing, now datetime: 2024-06-27 08:04:45.300102, test_epoch: 100\u001b[0m; \u001b[32m2024-06-27 08:04:45\u001b[0m \u001b[36mstart_test\u001b[0m:\u001b[36m42\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:46<00:00,  2.14epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtrain_test_util\u001b[0m: - \u001b[1mend testing, now datetime: 2024-06-27 08:05:32.049740\u001b[0m; \u001b[32m2024-06-27 08:05:32\u001b[0m \u001b[36mstart_test\u001b[0m:\u001b[36m46\u001b[0m\n",
      "avg reward: 272.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_policy_func = PolicyNetFunc.from_file(MODEL_PATH)\n",
    "_render_env = Env(gym.make(GYM_ENV_NAME, render_mode='rgb_array_list'))\n",
    "\n",
    "policy_func_tester = PolicyNetTester(\n",
    "    policy_fun=test_policy_func,\n",
    "    env=_render_env\n",
    ")\n",
    "\n",
    "\n",
    "avg_reward = start_test(\n",
    "    StandarTestProcess(\n",
    "        model=test_policy_func,\n",
    "        tester=policy_func_tester,\n",
    "        env=_render_env,\n",
    "        test_output_path=TEST_OUTPUT_PATH,\n",
    "        test_epoch=100,\n",
    "        show_result=False\n",
    "    )\n",
    ")\n",
    "print(f'avg reward: {avg_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数搜索： 使用CarPole-V1 环境，测试Policy-Based AC 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from policy_based import PolicyValueNetTrainer, ActionStateValueNetFunc, EpochEndCallback\n",
    "GYM_ENV_NAME = 'CartPole-v1'\n",
    "RESULT_DIR_NAME='cartpoleV1'\n",
    "\n",
    "_USE_CUDA = True and torch.cuda.is_available()\n",
    "# _USE_CUDA = False and torch.cuda.is_available()\n",
    "\n",
    "def objective(trial: optuna.Trial):\n",
    "    t_number = trial.number\n",
    "    env = Env.from_env_name(GYM_ENV_NAME)\n",
    "\n",
    "    LOG_PATH = Path(f'./run/logs/{RESULT_DIR_NAME}/{t_number}/AC')\n",
    "    MODEL_PATH = Path(f'./run/model/{RESULT_DIR_NAME}/AC_{t_number}/AC.pth')\n",
    "\n",
    "    \n",
    "    TRAIN_EPOCH = trial.suggest_int(name='train_epoch', low=1000, high=5000, step=1000)\n",
    "    HIDDEN_DIM_POLICY = trial.suggest_categorical(name='HIDDEN_DIM_POLICY', choices=[128, 256, 512])\n",
    "    HIDDEN_DIM_VALUE = trial.suggest_categorical(name='HIDDEN_DIM_VALUE', choices=[128, 256, 512])\n",
    "    \n",
    "    LEARNING_RATE = trial.suggest_float('p_learn_rate', low=5e-5, high=1e-1, log=True)\n",
    "    VLEARNING_RATE = trial.suggest_float('v_learn_rate', low=5e-5, high=1e-1, log=True)\n",
    "    GAMMA = trial.suggest_categorical(name='gamma', choices=[0.8, 0.9, 0.95, 0.99])\n",
    "\n",
    "\n",
    "    policy_func = PolicyNetFunc(env.get_state_dim()[0], \n",
    "                       action_nums=env.get_action_dim()[0], \n",
    "                       hidden_dim=HIDDEN_DIM_POLICY,\n",
    "                       device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "    value_func = ActionStateValueNetFunc(env.get_state_dim()[0],\n",
    "                              action_nums=env.get_action_dim()[0],\n",
    "                              hidden_dim=HIDDEN_DIM_VALUE,\n",
    "                              device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "    def epoch_end_callback(epoch, avg_reward, policy):\n",
    "        trial.report(avg_reward, epoch)\n",
    "        if epoch > 1000 and avg_reward < 200:\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    policy_func_trainer = PolicyValueNetTrainer(\n",
    "                                      policy_func=policy_func,\n",
    "                                      value_func=value_func,\n",
    "                                      vlearning_rate=VLEARNING_RATE,\n",
    "                                      env=env,\n",
    "                                      learning_rate=LEARNING_RATE,\n",
    "                                      gamma=GAMMA,\n",
    "                                      logger_folder=LOG_PATH,\n",
    "                                      epoch_end_callback=epoch_end_callback\n",
    "                                      )\n",
    "    \n",
    "    start_train(\n",
    "        StandarTrainProcess(\n",
    "            trainer=policy_func_trainer,\n",
    "            model=policy_func,\n",
    "            train_epoch=TRAIN_EPOCH,\n",
    "            log_path=LOG_PATH,\n",
    "            model_path=MODEL_PATH\n",
    "        )\n",
    "    )\n",
    "\n",
    "    policy_func_tester = PolicyNetTester(\n",
    "        policy_fun=policy_func,\n",
    "        env=env\n",
    "    )\n",
    "    \n",
    "    avg_reward = start_test(\n",
    "        StandarTestProcess(\n",
    "            model=test_policy_func,\n",
    "            tester=policy_func_tester,\n",
    "            env=env,\n",
    "            test_output_path=TEST_OUTPUT_PATH,\n",
    "            test_epoch=100,\n",
    "            show_result=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-27 09:49:40,952] A new study created in memory with name: no-name-bca75f0d-469d-4261-bef3-db8017c8bebe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtrain_test_util\u001b[0m: - \u001b[1mstart training, now datetime: 2024-06-27 09:49:40.958028\u001b[0m; \u001b[32m2024-06-27 09:49:40\u001b[0m \u001b[36mstart_train\u001b[0m:\u001b[36m62\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtrain_test_util\u001b[0m: - \u001b[1mFirst, clean log path: run\\logs\\cartpoleV1\\0\\AC, and clean model path: run\\model\\cartpoleV1\\AC_0\\AC.pth\u001b[0m; \u001b[32m2024-06-27 09:49:40\u001b[0m \u001b[36mstart_train\u001b[0m:\u001b[36m63\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtrain_test_util\u001b[0m: - \u001b[1mtrain started\u001b[0m; \u001b[32m2024-06-27 09:49:40\u001b[0m \u001b[36mstart_train\u001b[0m:\u001b[36m68\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1001/2000 [00:13<00:13, 73.51it/s, reward=9.00, step=9] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mtrain_test_util\u001b[0m: - \u001b[31m\u001b[1merror occured: \u001b[0m; \u001b[32m2024-06-27 09:49:54\u001b[0m \u001b[36mstart_train\u001b[0m:\u001b[36m74\u001b[0m\n",
      "\u001b[33m\u001b[1mTraceback (most recent call last):\u001b[0m\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "    │   └ <bound method Application.launch_instance of <class 'ipykernel.kernelapp.IPKernelApp'>>\n",
      "    └ <module 'ipykernel.kernelapp' from 'f:\\\\conda\\\\envs\\\\quant\\\\Lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "    │   └ <function IPKernelApp.start at 0x0000028BC44C2A20>\n",
      "    └ <ipykernel.kernelapp.IPKernelApp object at 0x0000028BBF22E450>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "    │    │       └ <function BaseAsyncIOLoop.start at 0x0000028BC44C3B00>\n",
      "    │    └ <tornado.platform.asyncio.AsyncIOMainLoop object at 0x0000028BC44E52D0>\n",
      "    └ <ipykernel.kernelapp.IPKernelApp object at 0x0000028BBF22E450>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "    │    │            └ <function BaseEventLoop.run_forever at 0x0000028BC1401E40>\n",
      "    │    └ <_WindowsSelectorEventLoop running=True closed=False debug=False>\n",
      "    └ <tornado.platform.asyncio.AsyncIOMainLoop object at 0x0000028BC44E52D0>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "    │    └ <function BaseEventLoop._run_once at 0x0000028BC1403C40>\n",
      "    └ <_WindowsSelectorEventLoop running=True closed=False debug=False>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "    │      └ <function Handle._run at 0x0000028BC0F909A0>\n",
      "    └ <Handle Task.task_wakeup(<Future finis...9B)>, ...],))>)>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "    │    │            │    │           │    └ <member '_args' of 'Handle' objects>\n",
      "    │    │            │    │           └ <Handle Task.task_wakeup(<Future finis...9B)>, ...],))>)>\n",
      "    │    │            │    └ <member '_callback' of 'Handle' objects>\n",
      "    │    │            └ <Handle Task.task_wakeup(<Future finis...9B)>, ...],))>)>\n",
      "    │    └ <member '_context' of 'Handle' objects>\n",
      "    └ <Handle Task.task_wakeup(<Future finis...9B)>, ...],))>)>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "          │    └ <function Kernel.process_one at 0x0000028BC30128E0>\n",
      "          └ <ipykernel.ipkernel.IPythonKernel object at 0x0000028BC44EA9D0>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "          │         └ ([<zmq.Frame(b'42f509c9-529'...36B)>, <zmq.Frame(b'<IDS|MSG>')>, <zmq.Frame(b'24f6ffc1e38d'...64B)>, <zmq.Frame(b'{\"date\":\"20...\n",
      "          └ <bound method Kernel.dispatch_shell of <ipykernel.ipkernel.IPythonKernel object at 0x0000028BC44EA9D0>>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "          └ <coroutine object IPythonKernel.execute_request at 0x0000028BD6EF7640>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 359, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "                                  │       │      └ {'header': {'date': datetime.datetime(2024, 6, 27, 1, 49, 40, 787000, tzinfo=tzutc()), 'msg_id': '3bb28be2-8209-4509-850b-38d...\n",
      "                                  │       └ [b'42f509c9-5295-4021-a958-dd0a72d23b66']\n",
      "                                  └ <zmq.eventloop.zmqstream.ZMQStream object at 0x0000028BC44E6690>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "                          └ <coroutine object IPythonKernel.do_execute at 0x0000028B8913E8E0>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 446, in do_execute\n",
      "    res = shell.run_cell(\n",
      "          │     └ <function ZMQInteractiveShell.run_cell at 0x0000028BC44A7BA0>\n",
      "          └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x0000028BC450C190>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "                             │       └ {'store_history': True, 'silent': False, 'cell_id': 'vscode-notebook-cell:/f%3A/ws/rf_learning/policy_based_learning.ipynb#X5...\n",
      "                             └ (\"study = optuna.create_study(direction='maximize')\\nstudy.optimize(objective, n_trials=100)\",)\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "             │    └ <function InteractiveShell._run_cell at 0x0000028BC265F100>\n",
      "             └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x0000028BC450C190>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "             │      └ <coroutine object InteractiveShell.run_cell_async at 0x0000028B97417E00>\n",
      "             └ <function _pseudo_sync_runner at 0x0000028BC26562A0>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "    │    └ <method 'send' of 'coroutine' objects>\n",
      "    └ <coroutine object InteractiveShell.run_cell_async at 0x0000028B97417E00>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "                       │    │             │        │     └ 'C:\\\\Users\\\\29000\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_1248\\\\3267595440.py'\n",
      "                       │    │             │        └ [<ast.Assign object at 0x0000028BD6F069E0>, <ast.Expr object at 0x0000028BD704B910>]\n",
      "                       │    │             └ <ast.Module object at 0x0000028BD6F060E0>\n",
      "                       │    └ <function InteractiveShell.run_ast_nodes at 0x0000028BC265F420>\n",
      "                       └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x0000028BC450C190>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "             │    │        │     │              └ False\n",
      "             │    │        │     └ <ExecutionResult object at 28bd6ee6190, execution_count=29 error_before_exec=None error_in_exec=None info=<ExecutionInfo obje...\n",
      "             │    │        └ <code object <module> at 0x0000028BD7016F10, file \"C:\\Users\\29000\\AppData\\Local\\Temp\\ipykernel_1248\\3267595440.py\", line 1>\n",
      "             │    └ <function InteractiveShell.run_code at 0x0000028BC265F4C0>\n",
      "             └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x0000028BC450C190>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "         │         │    │               │    └ {'__name__': '__main__', '__doc__': 'Automatically created module for IPython interactive environment', '__package__': None, ...\n",
      "         │         │    │               └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x0000028BC450C190>\n",
      "         │         │    └ <property object at 0x0000028BC2652C00>\n",
      "         │         └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x0000028BC450C190>\n",
      "         └ <code object <module> at 0x0000028BD7016F10, file \"C:\\Users\\29000\\AppData\\Local\\Temp\\ipykernel_1248\\3267595440.py\", line 1>\n",
      "\n",
      "  File \"\u001b[32mC:\\Users\\29000\\AppData\\Local\\Temp\\ipykernel_1248\\\u001b[0m\u001b[32m\u001b[1m3267595440.py\u001b[0m\", line \u001b[33m2\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    \u001b[1mstudy\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1moptimize\u001b[0m\u001b[1m(\u001b[0m\u001b[1mobjective\u001b[0m\u001b[1m,\u001b[0m \u001b[1mn_trials\u001b[0m\u001b[35m\u001b[1m=\u001b[0m\u001b[34m\u001b[1m100\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m│     │        └ \u001b[0m\u001b[36m\u001b[1m<function objective at 0x0000028BD6E5EFC0>\u001b[0m\n",
      "    \u001b[36m│     └ \u001b[0m\u001b[36m\u001b[1m<function Study.optimize at 0x0000028B9A71E8E0>\u001b[0m\n",
      "    \u001b[36m└ \u001b[0m\u001b[36m\u001b[1m<optuna.study.study.Study object at 0x0000028BD6EE7050>\u001b[0m\n",
      "\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\optuna\\study\\study.py\", line 451, in optimize\n",
      "    _optimize(\n",
      "    └ <function _optimize at 0x0000028B9A71C5E0>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 62, in _optimize\n",
      "    _optimize_sequential(\n",
      "    └ <function _optimize_sequential at 0x0000028B9A71C860>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 159, in _optimize_sequential\n",
      "    frozen_trial = _run_trial(study, func, catch)\n",
      "                   │          │      │     └ ()\n",
      "                   │          │      └ <function objective at 0x0000028BD6E5EFC0>\n",
      "                   │          └ <optuna.study.study.Study object at 0x0000028BD6EE7050>\n",
      "                   └ <function _run_trial at 0x0000028B9A71DDA0>\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      │    └ <optuna.trial._trial.Trial object at 0x0000028B9A814D50>\n",
      "                      └ <function objective at 0x0000028BD6E5EFC0>\n",
      "\n",
      "  File \"\u001b[32mC:\\Users\\29000\\AppData\\Local\\Temp\\ipykernel_1248\\\u001b[0m\u001b[32m\u001b[1m2355348299.py\u001b[0m\", line \u001b[33m52\u001b[0m, in \u001b[35mobjective\u001b[0m\n",
      "    \u001b[1mstart_train\u001b[0m\u001b[1m(\u001b[0m\n",
      "    \u001b[36m└ \u001b[0m\u001b[36m\u001b[1m<function start_train at 0x0000028B9A562840>\u001b[0m\n",
      "\n",
      "> File \"\u001b[32mf:\\ws\\rf_learning\\\u001b[0m\u001b[32m\u001b[1mtrain_test_util.py\u001b[0m\", line \u001b[33m69\u001b[0m, in \u001b[35mstart_train\u001b[0m\n",
      "    \u001b[1mtrain_process\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mtrainer\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mtrain\u001b[0m\u001b[1m(\u001b[0m\u001b[1mtrain_epoch\u001b[0m\u001b[35m\u001b[1m=\u001b[0m\u001b[1mtrain_process\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mtrain_epoch\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m│             │       │                 │             └ \u001b[0m\u001b[36m\u001b[1m2000\u001b[0m\n",
      "    \u001b[36m│             │       │                 └ \u001b[0m\u001b[36m\u001b[1mStandarTrainProcess(trainer=<policy_based.PolicyValueNetTrainer object at 0x0000028B9A8151D0>, model=PolicyNetFunc(\u001b[0m\n",
      "    \u001b[36m│             │       │                   \u001b[0m\u001b[36m\u001b[1m  (_fc1):...\u001b[0m\n",
      "    \u001b[36m│             │       └ \u001b[0m\u001b[36m\u001b[1m<function PolicyNetTrainer.train at 0x0000028B9A638900>\u001b[0m\n",
      "    \u001b[36m│             └ \u001b[0m\u001b[36m\u001b[1m<policy_based.PolicyValueNetTrainer object at 0x0000028B9A8151D0>\u001b[0m\n",
      "    \u001b[36m└ \u001b[0m\u001b[36m\u001b[1mStandarTrainProcess(trainer=<policy_based.PolicyValueNetTrainer object at 0x0000028B9A8151D0>, model=PolicyNetFunc(\u001b[0m\n",
      "    \u001b[36m  \u001b[0m\u001b[36m\u001b[1m  (_fc1):...\u001b[0m\n",
      "\n",
      "  File \"\u001b[32mf:\\ws\\rf_learning\\\u001b[0m\u001b[32m\u001b[1mpolicy_based.py\u001b[0m\", line \u001b[33m158\u001b[0m, in \u001b[35mtrain\u001b[0m\n",
      "    \u001b[1mself\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1m_epoch_end_callback\u001b[0m\u001b[1m(\u001b[0m\u001b[1mepoch\u001b[0m\u001b[1m,\u001b[0m \u001b[1macc_reward\u001b[0m\u001b[1m,\u001b[0m \u001b[1mself\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1m_policy_func\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m│    │                   │      │           │    └ \u001b[0m\u001b[36m\u001b[1mPolicyNetFunc(\u001b[0m\n",
      "    \u001b[36m│    │                   │      │           │      \u001b[0m\u001b[36m\u001b[1m  (_fc1): Linear(in_features=4, out_features=128, bias=True)\u001b[0m\n",
      "    \u001b[36m│    │                   │      │           │      \u001b[0m\u001b[36m\u001b[1m  (_fc2): Linear(in_features=128, out_features=2,...\u001b[0m\n",
      "    \u001b[36m│    │                   │      │           └ \u001b[0m\u001b[36m\u001b[1m<policy_based.PolicyValueNetTrainer object at 0x0000028B9A8151D0>\u001b[0m\n",
      "    \u001b[36m│    │                   │      └ \u001b[0m\u001b[36m\u001b[1m9.0\u001b[0m\n",
      "    \u001b[36m│    │                   └ \u001b[0m\u001b[36m\u001b[1m1001\u001b[0m\n",
      "    \u001b[36m│    └ \u001b[0m\u001b[36m\u001b[1m<function objective.<locals>.epoch_end_callback at 0x0000028BD70382C0>\u001b[0m\n",
      "    \u001b[36m└ \u001b[0m\u001b[36m\u001b[1m<policy_based.PolicyValueNetTrainer object at 0x0000028B9A8151D0>\u001b[0m\n",
      "\n",
      "  File \"\u001b[32mC:\\Users\\29000\\AppData\\Local\\Temp\\ipykernel_1248\\\u001b[0m\u001b[32m\u001b[1m2355348299.py\u001b[0m\", line \u001b[33m39\u001b[0m, in \u001b[35mepoch_end_callback\u001b[0m\n",
      "    \u001b[35m\u001b[1mraise\u001b[0m \u001b[1moptuna\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mTrialPruned\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m      │      └ \u001b[0m\u001b[36m\u001b[1m<class 'optuna.exceptions.TrialPruned'>\u001b[0m\n",
      "    \u001b[36m      └ \u001b[0m\u001b[36m\u001b[1m<module 'optuna' from 'f:\\\\conda\\\\envs\\\\quant\\\\Lib\\\\site-packages\\\\optuna\\\\__init__.py'>\u001b[0m\n",
      "\n",
      "\u001b[31m\u001b[1moptuna.exceptions.TrialPruned\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtrain_test_util\u001b[0m: - \u001b[1msaving model to: run\\model\\cartpoleV1\\AC_0\\AC.pth,\u001b[0m; \u001b[32m2024-06-27 09:49:54\u001b[0m \u001b[36mstart_train\u001b[0m:\u001b[36m76\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtrain_test_util\u001b[0m: - \u001b[1mstart testing, now datetime: 2024-06-27 09:49:54.586018, test_epoch: 100\u001b[0m; \u001b[32m2024-06-27 09:49:54\u001b[0m \u001b[36mstart_test\u001b[0m:\u001b[36m42\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100 [00:00<?, ?epoch/s]\n",
      "[W 2024-06-27 09:49:54,589] Trial 0 failed with parameters: {'train_epoch': 2000, 'HIDDEN_DIM_POLICY': 128, 'HIDDEN_DIM_VALUE': 256, 'p_learn_rate': 0.0046705415200356416, 'v_learn_rate': 9.441997456665225e-05, 'gamma': 0.95} because of the following error: RuntimeError('Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)').\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\29000\\AppData\\Local\\Temp\\ipykernel_1248\\2355348299.py\", line 67, in objective\n",
      "    avg_reward = start_test(\n",
      "                 ^^^^^^^^^^^\n",
      "  File \"f:\\ws\\rf_learning\\train_test_util.py\", line 44, in start_test\n",
      "    reward, _ = test_process.tester.test(1000)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"f:\\ws\\rf_learning\\policy_based.py\", line 328, in test\n",
      "    action = self._policy_func.get_optimal_action(torch.tensor(np.array([current_state]))).item()\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"f:\\ws\\rf_learning\\policy_based.py\", line 42, in get_optimal_action\n",
      "    out = self.forward(state)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"f:\\ws\\rf_learning\\policy_based.py\", line 33, in forward\n",
      "    x = torch.nn.functional.relu(self._fc1(x))\n",
      "                                 ^^^^^^^^^^^^\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1545, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1554, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"f:\\conda\\envs\\quant\\Lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 116, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "[W 2024-06-27 09:49:54,590] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\conda\\envs\\quant\\Lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\conda\\envs\\quant\\Lib\\site-packages\\optuna\\study\\_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mf:\\conda\\envs\\quant\\Lib\\site-packages\\optuna\\study\\_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mf:\\conda\\envs\\quant\\Lib\\site-packages\\optuna\\study\\_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    246\u001b[0m ):\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mf:\\conda\\envs\\quant\\Lib\\site-packages\\optuna\\study\\_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[28], line 67\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     52\u001b[0m start_train(\n\u001b[0;32m     53\u001b[0m     StandarTrainProcess(\n\u001b[0;32m     54\u001b[0m         trainer\u001b[38;5;241m=\u001b[39mpolicy_func_trainer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m     )\n\u001b[0;32m     60\u001b[0m )\n\u001b[0;32m     62\u001b[0m policy_func_tester \u001b[38;5;241m=\u001b[39m PolicyNetTester(\n\u001b[0;32m     63\u001b[0m     policy_fun\u001b[38;5;241m=\u001b[39mpolicy_func,\n\u001b[0;32m     64\u001b[0m     env\u001b[38;5;241m=\u001b[39menv\n\u001b[0;32m     65\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m avg_reward \u001b[38;5;241m=\u001b[39m \u001b[43mstart_test\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mStandarTestProcess\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_policy_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtester\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_func_tester\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_output_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTEST_OUTPUT_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m avg_reward\n",
      "File \u001b[1;32mf:\\ws\\rf_learning\\train_test_util.py:44\u001b[0m, in \u001b[0;36mstart_test\u001b[1;34m(test_process)\u001b[0m\n\u001b[0;32m     42\u001b[0m _logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart testing, now datetime: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, test_epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_process\u001b[38;5;241m.\u001b[39mtest_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(test_process\u001b[38;5;241m.\u001b[39mtest_epoch), unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 44\u001b[0m     reward, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtest_process\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtester\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m     reward_lst\u001b[38;5;241m.\u001b[39mappend(reward) \n\u001b[0;32m     46\u001b[0m _logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend testing, now datetime: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \n",
      "File \u001b[1;32mf:\\ws\\rf_learning\\policy_based.py:328\u001b[0m, in \u001b[0;36mPolicyNetTester.test\u001b[1;34m(self, max_step)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_step):\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stochastic:\n\u001b[1;32m--> 328\u001b[0m         action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_policy_func\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_optimal_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m         action_dis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy_func\u001b[38;5;241m.\u001b[39mget_action_distribute(torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray([current_state])))\n",
      "File \u001b[1;32mf:\\ws\\rf_learning\\policy_based.py:42\u001b[0m, in \u001b[0;36mPolicyNetFunc.get_optimal_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_optimal_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state: BatchedState) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchedAction:\n\u001b[1;32m---> 42\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32mf:\\ws\\rf_learning\\policy_based.py:33\u001b[0m, in \u001b[0;36mPolicyNetFunc.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 33\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     34\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fc2(x), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mf:\\conda\\envs\\quant\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1545\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1544\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\conda\\envs\\quant\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1554\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1550\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1552\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1553\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1557\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mf:\\conda\\envs\\quant\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
