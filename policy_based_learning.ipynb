{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "from policy_based import PolicyNetFunc, PolicyNetTrainer, PolicyNetTester\n",
    "from deep_q import Discrete1ContinuousAction\n",
    "from env import Env\n",
    "from utils import clear_target_path, show_gif_on_jupyternb, to_gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab1 Policy-Base Train\n",
    "\n",
    "使用CarPole-V1 环境，测试Policy-Based REINFORCE 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GYM_ENV_NAME = 'CartPole-v1'\n",
    "_train_gym_env = gym.make(GYM_ENV_NAME)\n",
    "env = Env(_train_gym_env)\n",
    "\n",
    "# 打印查看环境的动作空间和状态空间 \n",
    "action_nums, state_space = _train_gym_env.action_space.n, _train_gym_env.observation_space\n",
    "print(f'action num: {action_nums}, space: {state_space}')\n",
    "\n",
    "TRAIN_EPOCH = 1000\n",
    "HIDDEN_DIM = 256\n",
    "LEARNING_RATE = 2e-3\n",
    "GAMMA = 0.99\n",
    "\n",
    "\n",
    "LOG_PATH = Path('./run/logs/cartpoleV1/REINFORCE')\n",
    "MODEL_PATH = Path('./run/model/cartpoleV1/REINFORCE.pth')\n",
    "TEST_OUTPUT_PATH = Path('./run/test_result/cartpoleV1_REINFORCE')\n",
    "\n",
    "_USE_CUDA = True and torch.cuda.is_available()\n",
    "# _USE_CUDA = False and torch.cuda.is_available()\n",
    "\n",
    "policy_func = PolicyNetFunc(state_space.shape[0], \n",
    "                   action_nums, \n",
    "                   hidden_dim=HIDDEN_DIM, \n",
    "                   device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "\n",
    "policy_func_trainer = PolicyNetTrainer(policy_func=policy_func,\n",
    "                                  env=env,\n",
    "                                  learning_rate=LEARNING_RATE,\n",
    "                                  gamma=GAMMA,\n",
    "                                  logger_folder=LOG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_target_path(LOG_PATH)\n",
    "clear_target_path(MODEL_PATH)\n",
    "print(f'start training, now datetime: {datetime.datetime.now()}')\n",
    "policy_func_trainer.train(train_epoch=TRAIN_EPOCH)\n",
    "print(f'end training, saving model to: {MODEL_PATH}, now datetime: {datetime.datetime.now()}')\n",
    "\n",
    "policy_func.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_policy_func = PolicyNetFunc.from_file(MODEL_PATH)\n",
    "_render_env = Env(gym.make(GYM_ENV_NAME, render_mode='rgb_array_list'))\n",
    "policy_func_tester = PolicyNetTester(\n",
    "    policy_fun=test_policy_func.to('cpu'),\n",
    "    env=_render_env\n",
    ")\n",
    "RESULT_GIF = TEST_OUTPUT_PATH / 'result.gif'\n",
    "clear_target_path(RESULT_GIF)\n",
    "policy_func_tester.test(1000)\n",
    "to_gif(_render_env._gym_env, RESULT_GIF, 1/30)\n",
    "\n",
    "show_gif_on_jupyternb(RESULT_GIF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Pendulum-v1 环境，测试Policy-Based REINFORCE 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: Box(-2.0, 2.0, (1,), float32), space: Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)\n"
     ]
    }
   ],
   "source": [
    "GYM_ENV_NAME = 'Pendulum-v1'\n",
    "RESULT_DIR_NAME = 'pendulumV1'\n",
    "\n",
    "_train_gym_env = gym.make(GYM_ENV_NAME)\n",
    "env = Env(_train_gym_env)\n",
    "\n",
    "LOG_PATH = Path(f'./run/logs/{RESULT_DIR_NAME}/policy_based')\n",
    "MODEL_PATH = Path(f'./run/model/{RESULT_DIR_NAME}/policy_model.pth')\n",
    "TEST_OUTPUT_PATH = Path(f'./run/test_result/{RESULT_DIR_NAME}_policy')\n",
    "\n",
    "# 打印查看环境的动作空间和状态空间 \n",
    "action_space, state_space = _train_gym_env.action_space, _train_gym_env.observation_space\n",
    "print(f'action: {action_space}, space: {state_space}')\n",
    "\n",
    "# 动作空间离散化程度（用11个区间来替代连续动作空间）\n",
    "BINS = 100\n",
    "\n",
    "TRAIN_EPOCH = 10000\n",
    "HIDDEN_DIM = 512\n",
    "LEARNING_RATE = 1e-3\n",
    "GAMMA = 0.99\n",
    "\n",
    "_USE_CUDA = True and torch.cuda.is_available()\n",
    "# _USE_CUDA = False and torch.cuda.is_available()\n",
    "\n",
    "policy_func = PolicyNetFunc(state_space.shape[0], \n",
    "                   action_nums=BINS, \n",
    "                   hidden_dim=HIDDEN_DIM, \n",
    "                   device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "\n",
    "policy_func_trainer = PolicyNetTrainer(policy_func=policy_func,\n",
    "                                  env=env,\n",
    "                                  learning_rate=LEARNING_RATE,\n",
    "                                  gamma=GAMMA,\n",
    "                                  logger_folder=LOG_PATH,\n",
    "                                  action_converter=Discrete1ContinuousAction(action_space.low, action_space.high, BINS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training, now datetime: 2024-06-23 19:03:50.556291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter probs (Tensor of shape (1, 100)) of distribution Categorical(probs: torch.Size([1, 100])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([[-5.3898,  2.3251,  0.6364,  1.5145, -7.3999,  1.2428, -4.6396, -3.7236,\n          3.8088,  0.8192,  0.1155, -4.4931,  2.4001,  1.2866,  2.9567,  5.5142,\n          5.5776,  4.4540,  0.8457, -0.0687,  3.7172, -7.5980, -4.9409, -3.8109,\n         -2.1625, -0.0636, -4.1293,  1.0067,  2.0914, -0.0776,  0.4372, -4.9335,\n          2.4915, -0.1473, -3.7097, -0.0342,  0.4190, -0.5307, -0.1253,  5.7843,\n         -2.7124,  3.8801,  1.0071, -1.6388, -3.2328,  3.0431, -4.4419, -0.1577,\n          1.2578, -1.8795,  2.2531,  1.3166,  5.1002,  5.4249, -1.1794,  1.3867,\n         -1.8710,  6.1590, -0.5850,  0.8934, -3.3684,  0.0735, -2.1041, -4.4007,\n         -0.0197,  1.8454,  6.8711, -1.5800,  2.2488,  7.7269,  3.7603, -0.8571,\n          3.3215,  5.7406,  2.3631, -3.0928,  2.6089,  0.4041, -4.0942,  0.4256,\n         -3.3377, -7.4192,  0.7133,  0.7641, -0.8953, -5.0373, -3.0433,  0.6991,\n         -2.4090, -2.6097,  0.3464,  5.2790, -0.4483, -5.9656,  2.4049,  2.2071,\n         -1.3429,  0.8588, -0.1596, -2.9672]], grad_fn=<DivBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m clear_target_path(MODEL_PATH)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart training, now datetime: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mpolicy_func_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRAIN_EPOCH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend training, saving model to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, now datetime: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m policy_func\u001b[38;5;241m.\u001b[39msave(MODEL_PATH)\n",
      "File \u001b[1;32mf:\\ws\\rf_learning\\policy_based.py:163\u001b[0m, in \u001b[0;36mPolicyNetTrainer.train\u001b[1;34m(self, train_epoch)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# with torch.device(self._policy_func.get_device()):\u001b[39;00m\n\u001b[0;32m    162\u001b[0m action_distribute \u001b[38;5;241m=\u001b[39m l\u001b[38;5;241m.\u001b[39mget_action_distribute(torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray([current_state])))\n\u001b[1;32m--> 163\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_distribute\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    164\u001b[0m reward, next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mstep(\n\u001b[0;32m    165\u001b[0m     action \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_converter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_converter\u001b[38;5;241m.\u001b[39mto_continuous_action(action)\n\u001b[0;32m    166\u001b[0m     )\n\u001b[0;32m    167\u001b[0m acc_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[1;32mf:\\conda\\envs\\quant\\Lib\\site-packages\\torch\\distributions\\categorical.py:70\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     67\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n\u001b[0;32m     69\u001b[0m )\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\conda\\envs\\quant\\Lib\\site-packages\\torch\\distributions\\distribution.py:69\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     67\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m---> 69\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     70\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m             )\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mValueError\u001b[0m: Expected parameter probs (Tensor of shape (1, 100)) of distribution Categorical(probs: torch.Size([1, 100])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([[-5.3898,  2.3251,  0.6364,  1.5145, -7.3999,  1.2428, -4.6396, -3.7236,\n          3.8088,  0.8192,  0.1155, -4.4931,  2.4001,  1.2866,  2.9567,  5.5142,\n          5.5776,  4.4540,  0.8457, -0.0687,  3.7172, -7.5980, -4.9409, -3.8109,\n         -2.1625, -0.0636, -4.1293,  1.0067,  2.0914, -0.0776,  0.4372, -4.9335,\n          2.4915, -0.1473, -3.7097, -0.0342,  0.4190, -0.5307, -0.1253,  5.7843,\n         -2.7124,  3.8801,  1.0071, -1.6388, -3.2328,  3.0431, -4.4419, -0.1577,\n          1.2578, -1.8795,  2.2531,  1.3166,  5.1002,  5.4249, -1.1794,  1.3867,\n         -1.8710,  6.1590, -0.5850,  0.8934, -3.3684,  0.0735, -2.1041, -4.4007,\n         -0.0197,  1.8454,  6.8711, -1.5800,  2.2488,  7.7269,  3.7603, -0.8571,\n          3.3215,  5.7406,  2.3631, -3.0928,  2.6089,  0.4041, -4.0942,  0.4256,\n         -3.3377, -7.4192,  0.7133,  0.7641, -0.8953, -5.0373, -3.0433,  0.6991,\n         -2.4090, -2.6097,  0.3464,  5.2790, -0.4483, -5.9656,  2.4049,  2.2071,\n         -1.3429,  0.8588, -0.1596, -2.9672]], grad_fn=<DivBackward0>)"
     ]
    }
   ],
   "source": [
    "clear_target_path(LOG_PATH)\n",
    "clear_target_path(MODEL_PATH)\n",
    "print(f'start training, now datetime: {datetime.datetime.now()}')\n",
    "policy_func_trainer.train(train_epoch=TRAIN_EPOCH)\n",
    "print(f'end training, saving model to: {MODEL_PATH}, now datetime: {datetime.datetime.now()}')\n",
    "\n",
    "policy_func.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_policy_func = PolicyNetFunc.from_file(MODEL_PATH)\n",
    "_render_env = Env(gym.make(GYM_ENV_NAME, render_mode='rgb_array_list'))\n",
    "policy_func_tester = PolicyNetTester(\n",
    "    policy_fun=test_policy_func.to('cpu'),\n",
    "    env=_render_env,\n",
    "    action_converter=Discrete1ContinuousAction(action_space.low, action_space.high, BINS)\n",
    ")\n",
    "RESULT_GIF = TEST_OUTPUT_PATH / 'result.gif'\n",
    "clear_target_path(RESULT_GIF)\n",
    "policy_func_tester.test(1000)\n",
    "to_gif(_render_env._gym_env, RESULT_GIF, 1/30)\n",
    "\n",
    "show_gif_on_jupyternb(RESULT_GIF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Pendulum-v1 环境，测试Policy-Based AC 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: Box(-2.0, 2.0, (1,), float32), space: Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)\n"
     ]
    }
   ],
   "source": [
    "from policy_based import PolicyValueNetTrainer, ValueNetFunc\n",
    "\n",
    "GYM_ENV_NAME = 'Pendulum-v1'\n",
    "RESULT_DIR_NAME = 'pendulumV1'\n",
    "\n",
    "_train_gym_env = gym.make(GYM_ENV_NAME)\n",
    "env = Env(_train_gym_env)\n",
    "\n",
    "LOG_PATH = Path(f'./run/logs/{RESULT_DIR_NAME}/AC')\n",
    "MODEL_PATH = Path(f'./run/model/{RESULT_DIR_NAME}/AC.pth')\n",
    "TEST_OUTPUT_PATH = Path(f'./run/test_result/{RESULT_DIR_NAME}_AC')\n",
    "\n",
    "# 打印查看环境的动作空间和状态空间 \n",
    "action_space, state_space = _train_gym_env.action_space, _train_gym_env.observation_space\n",
    "print(f'action: {action_space}, space: {state_space}')\n",
    "\n",
    "# 动作空间离散化程度（用11个区间来替代连续动作空间）\n",
    "BINS = 100\n",
    "\n",
    "TRAIN_EPOCH = 10000\n",
    "HIDDEN_DIM = 512\n",
    "LEARNING_RATE = 1e-3\n",
    "GAMMA = 0.99\n",
    "\n",
    "_USE_CUDA = True and torch.cuda.is_available()\n",
    "# _USE_CUDA = False and torch.cuda.is_available()\n",
    "\n",
    "policy_func = PolicyNetFunc(state_space.shape[0], \n",
    "                   action_nums=BINS, \n",
    "                   hidden_dim=HIDDEN_DIM, \n",
    "                   device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "value_func = ValueNetFunc(state_space.shape[0],\n",
    "                          action_nums=BINS,\n",
    "                          hidden_dim=HIDDEN_DIM,\n",
    "                          device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "\n",
    "policy_func_trainer = PolicyValueNetTrainer(\n",
    "                                  policy_func=policy_func,\n",
    "                                  value_func=value_func,\n",
    "                                  env=env,\n",
    "                                  learning_rate=LEARNING_RATE,\n",
    "                                  gamma=GAMMA,\n",
    "                                  logger_folder=LOG_PATH,\n",
    "                                  action_converter=Discrete1ContinuousAction(action_space.low, action_space.high, BINS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training, now datetime: 2024-06-23 18:57:12.444726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter probs (Tensor of shape (1, 100)) of distribution Categorical(probs: torch.Size([1, 100])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([[ 0.2406,  0.0961,  0.1337,  0.0782, -0.1154, -0.1222,  0.1802,  0.0244,\n          0.0349,  0.0377,  0.0848,  0.1287, -0.0695, -0.1559,  0.1158,  0.1033,\n          0.0769, -0.0256, -0.1743, -0.0521, -0.1410,  0.0686, -0.1318,  0.0248,\n          0.0215,  0.1120,  0.0047,  0.0177, -0.0195,  0.0919,  0.1800, -0.2526,\n         -0.0118, -0.0618,  0.0461,  0.0384, -0.1582, -0.1590,  0.0863, -0.1202,\n          0.1166, -0.1542,  0.1339,  0.0280, -0.0096, -0.0305,  0.2425,  0.0803,\n          0.1145, -0.0994,  0.0407,  0.0984, -0.0208, -0.1882,  0.0378,  0.1055,\n          0.0508,  0.3263, -0.1178,  0.1769,  0.0296,  0.0965, -0.0600,  0.0566,\n         -0.0968,  0.1186,  0.0347, -0.0248,  0.0909, -0.2014, -0.0301,  0.1220,\n          0.0784,  0.0249,  0.0017, -0.2301, -0.1467,  0.0487, -0.1127,  0.0840,\n          0.0855,  0.0126, -0.1214,  0.2169, -0.1897, -0.0806,  0.1497,  0.2023,\n          0.0372, -0.1580,  0.1418,  0.0940, -0.0623, -0.0199,  0.0118, -0.0523,\n         -0.2351,  0.0395, -0.1983,  0.0548]], grad_fn=<DivBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m clear_target_path(MODEL_PATH)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart training, now datetime: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mpolicy_func_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRAIN_EPOCH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend training, saving model to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, now datetime: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m policy_func\u001b[38;5;241m.\u001b[39msave(MODEL_PATH)\n",
      "File \u001b[1;32mf:\\ws\\rf_learning\\policy_based.py:163\u001b[0m, in \u001b[0;36mPolicyNetTrainer.train\u001b[1;34m(self, train_epoch)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# with torch.device(self._policy_func.get_device()):\u001b[39;00m\n\u001b[0;32m    162\u001b[0m action_distribute \u001b[38;5;241m=\u001b[39m l\u001b[38;5;241m.\u001b[39mget_action_distribute(torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray([current_state])))\n\u001b[1;32m--> 163\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_distribute\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    164\u001b[0m reward, next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mstep(\n\u001b[0;32m    165\u001b[0m     action \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_converter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_converter\u001b[38;5;241m.\u001b[39mto_continuous_action(action)\n\u001b[0;32m    166\u001b[0m     )\n\u001b[0;32m    167\u001b[0m acc_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[1;32mf:\\conda\\envs\\quant\\Lib\\site-packages\\torch\\distributions\\categorical.py:70\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     67\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n\u001b[0;32m     69\u001b[0m )\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\conda\\envs\\quant\\Lib\\site-packages\\torch\\distributions\\distribution.py:69\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     67\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m---> 69\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     70\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m             )\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mValueError\u001b[0m: Expected parameter probs (Tensor of shape (1, 100)) of distribution Categorical(probs: torch.Size([1, 100])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([[ 0.2406,  0.0961,  0.1337,  0.0782, -0.1154, -0.1222,  0.1802,  0.0244,\n          0.0349,  0.0377,  0.0848,  0.1287, -0.0695, -0.1559,  0.1158,  0.1033,\n          0.0769, -0.0256, -0.1743, -0.0521, -0.1410,  0.0686, -0.1318,  0.0248,\n          0.0215,  0.1120,  0.0047,  0.0177, -0.0195,  0.0919,  0.1800, -0.2526,\n         -0.0118, -0.0618,  0.0461,  0.0384, -0.1582, -0.1590,  0.0863, -0.1202,\n          0.1166, -0.1542,  0.1339,  0.0280, -0.0096, -0.0305,  0.2425,  0.0803,\n          0.1145, -0.0994,  0.0407,  0.0984, -0.0208, -0.1882,  0.0378,  0.1055,\n          0.0508,  0.3263, -0.1178,  0.1769,  0.0296,  0.0965, -0.0600,  0.0566,\n         -0.0968,  0.1186,  0.0347, -0.0248,  0.0909, -0.2014, -0.0301,  0.1220,\n          0.0784,  0.0249,  0.0017, -0.2301, -0.1467,  0.0487, -0.1127,  0.0840,\n          0.0855,  0.0126, -0.1214,  0.2169, -0.1897, -0.0806,  0.1497,  0.2023,\n          0.0372, -0.1580,  0.1418,  0.0940, -0.0623, -0.0199,  0.0118, -0.0523,\n         -0.2351,  0.0395, -0.1983,  0.0548]], grad_fn=<DivBackward0>)"
     ]
    }
   ],
   "source": [
    "clear_target_path(LOG_PATH)\n",
    "clear_target_path(MODEL_PATH)\n",
    "print(f'start training, now datetime: {datetime.datetime.now()}')\n",
    "policy_func_trainer.train(train_epoch=TRAIN_EPOCH)\n",
    "print(f'end training, saving model to: {MODEL_PATH}, now datetime: {datetime.datetime.now()}')\n",
    "\n",
    "policy_func.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
