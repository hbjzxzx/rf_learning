{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "from deep_q import DeepQFunc, DeepQFuncTrainer, DeepQFuncTester, ReplayBuffer, Discrete1ContinuousAction\n",
    "from env import Env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用CarPole-V1 环境，测试简单的Deep Q 如何处理连续的State空间和离散的Action空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GYM_ENV_NAME = 'CartPole-v1'\n",
    "_train_gym_env = gym.make(GYM_ENV_NAME)\n",
    "\n",
    "# 打印查看环境的动作空间和状态空间 \n",
    "action_nums, state_space = _train_gym_env.action_space.n, _train_gym_env.observation_space\n",
    "print(f'action num: {action_nums}, space: {state_space}')\n",
    "\n",
    "TRAIN_EPOCH = 300\n",
    "HIDDEN_DIM = 256\n",
    "LEARNING_RATE = 2e-3\n",
    "GAMMA = 0.99\n",
    "\n",
    "# 使用指数递减的epsilon-greedy策略\n",
    "START_EPSILON = 0.5\n",
    "END_EPSILON = 0.05\n",
    "DECAY_RATE = 0.99\n",
    "EPSILON_LIST = [max(START_EPSILON * (DECAY_RATE ** i), END_EPSILON) for i in range(TRAIN_EPOCH)]\n",
    "\n",
    "\n",
    "log_path = Path('./logs/run2')\n",
    "if log_path.exists():\n",
    "    shutil.rmtree(log_path)\n",
    "\n",
    "# _USE_CUDA = True and torch.cuda.is_available()\n",
    "_USE_CUDA = False and torch.cuda.is_available()\n",
    "\n",
    "q_func = DeepQFunc(state_space.shape[0], \n",
    "                   action_nums, \n",
    "                   hidden_dim=HIDDEN_DIM, \n",
    "                   device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "env = Env(_train_gym_env)\n",
    "\n",
    "replay_buffer = ReplayBuffer(10000)\n",
    "q_func_trainer = DeepQFuncTrainer(q_func=q_func, \n",
    "                                  env=env,\n",
    "                                  replay_buffer=replay_buffer,\n",
    "                                  learning_rate=LEARNING_RATE,\n",
    "                                  batch_size=64,\n",
    "                                  gamma=GAMMA,\n",
    "                                  epsilon_list=EPSILON_LIST,\n",
    "                                  logger_folder=log_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_func_trainer.train(train_epoch=TRAIN_EPOCH, \n",
    "                     max_steps=1000, \n",
    "                     minimal_replay_size_to_train=64 * 10,\n",
    "                     target_q_update_freq=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_render_env = Env(gym.make(GYM_ENV_NAME, render_mode='human'))\n",
    "q_func_tester = DeepQFuncTester(\n",
    "    q_func=q_func.to('cpu'),\n",
    "    env=_render_env\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_func_tester.test(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path('./model')\n",
    "if not p.exists():\n",
    "    p.mkdir()\n",
    "q_func.save(Path('./model/trained_for_cartpole.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_func_from_checkpoint = DeepQFunc(state_space.shape[0], \n",
    "                   action_nums, \n",
    "                   hidden_dim=HIDDEN_DIM)\n",
    "q_func_from_checkpoint.load(Path('./model/trained_for_cartpole.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_func_tester = DeepQFuncTester(\n",
    "    q_func=q_func_from_checkpoint.to('cpu'),\n",
    "    env=_render_env\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_func_tester.test(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用Pendulum-v1 测试Double Q Learning对Q值系统高估的处理能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: Box(-2.0, 2.0, (1,), float32), space: Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)\n"
     ]
    }
   ],
   "source": [
    "GYM_ENV_NAME = 'Pendulum-v1'\n",
    "_train_gym_env = gym.make(GYM_ENV_NAME)\n",
    "\n",
    "# 打印查看环境的动作空间和状态空间 \n",
    "action_space, state_space = _train_gym_env.action_space, _train_gym_env.observation_space\n",
    "print(f'action: {action_space}, space: {state_space}')\n",
    "\n",
    "BINS = 11\n",
    "\n",
    "\n",
    "TRAIN_EPOCH = 1000\n",
    "HIDDEN_DIM = 128\n",
    "LEARNING_RATE = 2e-3\n",
    "GAMMA = 0.99\n",
    "\n",
    "# 使用指数递减的epsilon-greedy策略\n",
    "START_EPSILON = 0.5\n",
    "END_EPSILON = 0.05\n",
    "DECAY_RATE = 0.99\n",
    "EPSILON_LIST = [max(START_EPSILON * (DECAY_RATE ** i), END_EPSILON) for i in range(TRAIN_EPOCH)]\n",
    "\n",
    "\n",
    "log_path = Path('./logs/pendulum/run_dqn')\n",
    "if log_path.exists():\n",
    "    shutil.rmtree(log_path)\n",
    "\n",
    "# _USE_CUDA = True and torch.cuda.is_available()\n",
    "_USE_CUDA = False and torch.cuda.is_available()\n",
    "\n",
    "q_func = DeepQFunc(state_space.shape[0], \n",
    "                   BINS, \n",
    "                   hidden_dim=HIDDEN_DIM, \n",
    "                   device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "env = Env(_train_gym_env)\n",
    "\n",
    "replay_buffer = ReplayBuffer(10000)\n",
    "q_func_trainer = DeepQFuncTrainer(q_func=q_func, \n",
    "                                  env=env,\n",
    "                                  replay_buffer=replay_buffer,\n",
    "                                  learning_rate=LEARNING_RATE,\n",
    "                                  batch_size=64,\n",
    "                                  gamma=GAMMA,\n",
    "                                  epsilon_list=EPSILON_LIST,\n",
    "                                  logger_folder=log_path,\n",
    "                                  action_converter=Discrete1ContinuousAction(action_space.low, action_space.high, BINS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_func_trainer.train(train_epoch=TRAIN_EPOCH, \n",
    "                     max_steps=1000, \n",
    "                     minimal_replay_size_to_train=64 * 10,\n",
    "                     target_q_update_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_render_env = Env(gym.make(GYM_ENV_NAME, render_mode='human'))\n",
    "q_func_tester = DeepQFuncTester(\n",
    "    q_func=q_func.to('cpu'),\n",
    "    env=_render_env,\n",
    "    action_converter=Discrete1ContinuousAction(action_space.low, action_space.high, BINS)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_func_tester.test(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用Double DQN 进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_q import DoubleQFuncTrainer\n",
    "\n",
    "log_path = Path('./logs/pendulum/run_double_dqn')\n",
    "if log_path.exists():\n",
    "    shutil.rmtree(log_path)\n",
    "\n",
    "q_func2 = DeepQFunc(state_space.shape[0], \n",
    "                   BINS, \n",
    "                   hidden_dim=HIDDEN_DIM, \n",
    "                   device=torch.device('cuda') if _USE_CUDA else None)\n",
    "\n",
    "replay_buffer = ReplayBuffer(10000)\n",
    "q_func_trainer = DoubleQFuncTrainer(q_func=q_func2, \n",
    "                                  env=env,\n",
    "                                  replay_buffer=replay_buffer,\n",
    "                                  learning_rate=LEARNING_RATE,\n",
    "                                  batch_size=64,\n",
    "                                  gamma=GAMMA,\n",
    "                                  epsilon_list=EPSILON_LIST,\n",
    "                                  logger_folder=log_path,\n",
    "                                  action_converter=Discrete1ContinuousAction(action_space.low, action_space.high, BINS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]f:\\conda\\envs\\quant\\Lib\\site-packages\\torch\\utils\\_device.py:78: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  return func(*args, **kwargs)\n",
      "  0%|          | 3/1000 [00:00<00:48, 20.41it/s, reward=-1.6e+3, step=200]f:\\ws\\rf_learning\\deep_q.py:294: UserWarning: Using a target size (torch.Size([64, 64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = torch.nn.functional.mse_loss(q_values_now_value, target_q_values)\n",
      "100%|██████████| 1000/1000 [05:08<00:00,  3.24it/s, reward=-1.52e+3, step=200]\n"
     ]
    }
   ],
   "source": [
    "q_func_trainer.train(train_epoch=TRAIN_EPOCH, \n",
    "                     max_steps=1000, \n",
    "                     minimal_replay_size_to_train=64 * 10,\n",
    "                     target_q_update_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_render_env = Env(gym.make(GYM_ENV_NAME, render_mode='human'))\n",
    "q_func_tester = DeepQFuncTester(\n",
    "    q_func=q_func2.to('cpu'),\n",
    "    env=_render_env,\n",
    "    action_converter=Discrete1ContinuousAction(action_space.low, action_space.high, BINS)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test reward: -1587.9141391611133\n",
      "Step Rewards: [-8.200818814300813, -8.293559493080187, -8.3735609796617, -8.438001719024937, -8.484637731720484, -8.511874468543352, -8.51881284377955, -8.505269923449484, -8.47177560906836, -8.419547058659036, -8.350442645728188, -8.266897147406864, -8.171839761819966, -8.0685966432735, -7.960780009415742, -7.852166518544274, -7.746568431183363, -7.647701865637723, -7.559057012450378, -7.483775317189709, -7.4245383196688675, -7.383472134708622, -7.362070667419733, -7.361139800983642, -7.380764148733194, -7.42029757495848, -7.478378468161167, -7.552970495953539, -7.641429050047052, -7.740592632343593, -7.846897009927621, -7.956508225852184, -8.065468794622518, -8.169850007276391, -8.26590254862307, -8.350197767901646, -8.419752918766507, -8.472135278945116, -8.505541941388092, -8.518853890496437, -8.511664468894622, -8.484283359311213, -8.437717742151285, -8.37363243792946, -8.294290766497317, -8.202477738884687, -8.10140721598634, -7.994614942493837, -7.8858399282349865, -7.77889742894834, -7.677547604195483, -7.585364582312038, -7.5056109469445165, -7.441122480836242, -7.394207401638864, -7.366563466835761, -7.359215435682751, -7.372474652912873, -7.405922054395247, -7.45841563996098, -7.528123239857343, -7.612580986447251, -7.708777092260955, -7.813259249845485, -7.922262300628561, -8.031851028753582, -8.138071382114804, -8.237102460361545, -8.325401472952628, -8.39983459993588, -8.457788121734326, -8.49725601489527, -8.516902085103599, -8.516096325137342, -8.494926358750975, -8.454185511877203, -8.395339306698396, -8.320472147583068, -8.232215843886342, -8.13366157493546, -8.028257088733366, -7.919691402228786, -7.81176999791002, -7.708284345418948, -7.612880306675672, -7.528930398186264, -7.459414851890615, -7.406815933632035, -7.373029177606255, -7.359294291524185, -7.3661476963390715, -7.393398117767241, -7.440126340273094, -7.50471002799102, -7.584874188102238, -7.67776718048912, -7.78006103839067, -7.8880732948916, -7.997905721327971, -8.105593715399591, -8.207258897505966, -8.299257065740816, -8.378314128336768, -8.44164387461184, -8.48704319033079, -8.512962220591874, -8.518548706146728, -8.50366704190876, -8.468893437946093, -8.415488937404653, -8.34535208798272, -8.26095294960326, -8.165250037168034, -8.061591901967248, -7.953605440371028, -7.845073678192837, -7.739806600322837, -7.641509381619573, -7.553652907471016, -7.479351586420189, -7.4212531067207825, -7.381444066474998, -7.361374510494012, -7.3618035615592685, -7.382767704667419, -7.423572910760597, -7.482811570507591, -7.558404943605122, -7.647671283594426, -7.747418809088104, -7.854061241656856, -7.963751878031866, -8.072530418039802, -8.176475401653633, -8.27185443950567, -8.355264618083798, -8.42375648474758, -8.474936642004707, -8.50704586827525, -8.519011489484733, -8.5104741894418, -8.481790433184933, -8.434012184088303, -8.368845723759794, -8.288591298157941, -8.196065201134516, -8.094505938930205, -7.987466412480776, -7.878694635851258, -7.77200629810787, -7.671153297079603, -7.579693006683742, -7.500863295144663, -7.437468101914525, -7.391777757129124, -7.3654473605563675, -7.359455653477888, -7.374066109444333, -7.408811522178654, -7.462503122724327, -7.533265033564386, -7.618594431764243, -7.715446952101917, -7.820345545436162, -7.929509321663462, -8.038997119358275, -8.144859019288244, -8.243288105584613, -8.330764699830254, -8.404186076323832, -8.460976137069851, -8.499171369381319, -8.517481274580922, -8.515323047684083, -8.492831428871966, -8.450845296746971, -8.390872803919102, -8.315036816667012, -8.22600229661726, -8.126887233894598, -8.021158945583014, -7.9125180485924345, -7.80477315542225, -7.701710175293483, -7.6069608182019355, -7.5238752879927615, -7.455404087095842, -7.403993346965337, -7.37149728364352, -7.359110475552895, -7.3673218830975875, -7.395892000587502, -7.443854236642109, -7.509541412279953, -7.590637920551721, -7.684257391409226, -7.787044534912318, -7.895298243259914, -8.005111239956351, -8.112519917354847]\n"
     ]
    }
   ],
   "source": [
    "q_func_tester.test(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
