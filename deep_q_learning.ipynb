{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 考虑连续State空间、离散Action 空间的Q函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Tuple, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "State = int\n",
    "# 表示状态特征向量的维度\n",
    "StateDim = int\n",
    "StateVec = List[float]\n",
    "\n",
    "Action = int\n",
    "Reward = float\n",
    "ActionProbDistribution = List[float]\n",
    "\n",
    "class AbstractQFunc():\n",
    "    def get_value(self, state: State, action: Action) -> float:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def get_action_distribute(self, state: State) -> ActionProbDistribution:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_actions_count(self) -> int:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def set_value(self, state: State, action: Action, value: float) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class DeepQFunc(AbstractQFunc, torch.nn.Module):\n",
    "    def __init__(self, state_dim: int, action_nums: int, hidden_dim: int = 128) -> None:\n",
    "        # here use full-connect layer to represent Q function\n",
    "        super().__init__() \n",
    "        self._state_dims = state_dim \n",
    "        self._action_nums = action_nums\n",
    "        \n",
    "        self._fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self._fc2 = torch.nn.Linear(hidden_dim, action_nums)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = torch.nn.functional.relu(self._fc1(x))\n",
    "        return self._fc2(x)\n",
    "        \n",
    "    def get_action_distribute(self, state: State) -> ActionProbDistribution:\n",
    "        out = self.forward(torch.tensor([state]))\n",
    "        return torch.nn.functional.softmax(out, dim=0).detach().numpy()\n",
    "\n",
    "    def get_optimal_action(self, state: State) -> Action:    \n",
    "        out = self.forward(torch.tensor([state]))\n",
    "        return torch.argmax(out).item()\n",
    "\n",
    "    def get_actions_count(self) -> int:\n",
    "        return self._action_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self, gym_env: gym.Env):\n",
    "        self._gym_env = gym_env\n",
    "\n",
    "    def step(self, action: Action) ->  Tuple[Reward, Optional[State]]: \n",
    "        next_state, reward, is_terminated, is_truncated, _ = self._gym_env.step(action)\n",
    "        if is_terminated or is_truncated:\n",
    "            return reward, None\n",
    "        else:\n",
    "            return reward, next_state\n",
    "\n",
    "    def reset(self) -> State:\n",
    "        init_state, _ = self._gym_env.reset()\n",
    "        return init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 策略函数\n",
    "# todo: change the right type\n",
    "ActionProbDistribution = List[float]\n",
    "Strategy = Callable[[State], ActionProbDistribution]\n",
    "\n",
    "\n",
    "def to_strategy(f: DeepQFunc) -> Strategy:\n",
    "    def _strategy(s: State) -> ActionProbDistribution:\n",
    "        x = f.get_action_distribute(s).detach().numpy()\n",
    "        return x\n",
    "    return _strategy\n",
    "\n",
    "def to_strategy_epsilon_greedy(f: DeepQFunc, epsilon: float) -> Strategy:\n",
    "    def _strategy(s: State) -> ActionProbDistribution:\n",
    "        # e-greedy 策略\n",
    "        if np.random.uniform(0, 1) > epsilon:\n",
    "            # 这里选择最优动作（没有随机性）\n",
    "            optimal_action = f.get_optimal_action(s)\n",
    "            # 创建一个one-hot编码的动作分布\n",
    "            action_distribution = np.zeros(f.get_actions_count(), dtype=np.float32)\n",
    "            action_distribution[optimal_action] = 1.0\n",
    "            return action_distribution\n",
    "        else:\n",
    "            # 随机选择动作 \n",
    "            return np.ones(f.get_actions_count(), dtype=np.float32) / f.get_actions_count()\n",
    "    return _strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, weight):\n",
    "        self.buffer.append((state, action, reward, next_state, weight))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        return zip(*transitions)        \n",
    "\n",
    "class DQFuncTrainer():\n",
    "    def __init__(self, q_func: DeepQFunc, \n",
    "                 env: Env, \n",
    "                 replay_buffer: ReplayBuffer,\n",
    "                 optimizer: optim.Optimizer,\n",
    "                 batch_size: int,\n",
    "                 gamma: float,\n",
    "                 epsilon_list: List[float],\n",
    "                 logger_folder: Optional[Path] = None,\n",
    "                 ) -> None:\n",
    "        self._q_func = q_func\n",
    "        self._env = env\n",
    "        self._replay_buffer = replay_buffer\n",
    "        self._gamma = gamma\n",
    "        self._optimizer = optimizer\n",
    "        self._batch_size = batch_size\n",
    "        self._epsilon_list = epsilon_list\n",
    "\n",
    "        self._logger_folder = logger_folder if logger_folder is not None else Path('./logs')\n",
    "\n",
    "    def train(self, epoch: int, max_steps: int, minimal_train_size: int):\n",
    "        writer = SummaryWriter(self._logger_folder) \n",
    "        for epoch in tqdm(range(epoch)):\n",
    "            init_state = self._env.reset()\n",
    "            current_state = init_state\n",
    "            acc_reward = 0\n",
    "            step_cnt = 0\n",
    "            \n",
    "            for _s in range(max_steps):\n",
    "                step_cnt += 1\n",
    "                # 获取此时DeepQFunc的策略 \n",
    "                e_greedy_s = to_strategy_epsilon_greedy(self._q_func, self._epsilon_list[epoch])\n",
    "                # 使用该策略进行决策 \n",
    "                action_dist = e_greedy_s(current_state)\n",
    "                action = np.random.choice(self._q_func.get_actions_count(), p=action_dist)\n",
    "                # 执行这个动作，获取下一个状态      \n",
    "                reward, next_state = self._env.step(action)\n",
    "                acc_reward += reward\n",
    "                \n",
    "                if next_state is not None:\n",
    "                    self._replay_buffer.add(current_state, action, reward, next_state, 1)\n",
    "                else:\n",
    "                    self._replay_buffer.add(current_state, action, reward, current_state, 0)\n",
    "                current_state = next_state\n",
    "                if len(self._replay_buffer.buffer) > minimal_train_size:\n",
    "                    self.update_q_func()\n",
    "\n",
    "                if current_state is None: \n",
    "                    break\n",
    "            writer.add_scalar('reward', acc_reward, epoch)\n",
    "            writer.add_scalar('step', step_cnt, epoch)\n",
    "    \n",
    "    def update_q_func(self):\n",
    "        state, action, reward, next_state, weight = self._replay_buffer.sample(self._batch_size)\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        action = torch.tensor(action, dtype=torch.int64)\n",
    "        reward = torch.tensor(reward, dtype=torch.float32)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "        weight = torch.tensor(weight, dtype=torch.int)\n",
    "        \n",
    "        q_values = self._q_func(state)\n",
    "        next_q_values = self._q_func(next_state)\n",
    "        target_q_values = reward + self._gamma * torch.max(next_q_values, dim=1).values * weight\n",
    "        target_q_values = target_q_values.detach()\n",
    "        loss = torch.nn.functional.mse_loss(q_values.gather(1, action.unsqueeze(1)), target_q_values.unsqueeze(1))\n",
    "        \n",
    "        self._optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self._optimizer.step()\n",
    "\n",
    "class DoubleQ_DQFuncTrainer():\n",
    "    def __init__(self, \n",
    "                 q_func: DeepQFunc,\n",
    "                 target_q_func: DeepQFunc, \n",
    "                 env: Env, \n",
    "                 replay_buffer: ReplayBuffer,\n",
    "                 optimizer: optim.Optimizer,\n",
    "                 batch_size: int,\n",
    "                 gamma: float,\n",
    "                 epsilon_list: List[float],\n",
    "                 target_update_freq: int,\n",
    "                 logger_folder: Optional[Path] = None,\n",
    "                 ) -> None:\n",
    "        self._q_func = q_func\n",
    "        self._target_q_func = target_q_func\n",
    "        self._env = env\n",
    "        self._replay_buffer = replay_buffer\n",
    "        self._gamma = gamma\n",
    "        self._optimizer = optimizer\n",
    "        self._batch_size = batch_size\n",
    "        self._epsilon_list = epsilon_list\n",
    "        self._target_update_freq = target_update_freq\n",
    "\n",
    "        self._logger_folder = logger_folder if logger_folder is not None else Path('./logs')\n",
    "\n",
    "\n",
    "        self.__update_count = 0\n",
    "    def train(self, epoch: int, max_steps: int, minimal_train_size: int):\n",
    "        writer = SummaryWriter(self._logger_folder) \n",
    "        for epoch in tqdm(range(epoch)):\n",
    "            init_state = self._env.reset()\n",
    "            current_state = init_state\n",
    "            acc_reward = 0\n",
    "            step_cnt = 0\n",
    "            \n",
    "            for _s in range(max_steps):\n",
    "                step_cnt += 1\n",
    "                # 获取此时DeepQFunc的策略 \n",
    "                e_greedy_s = to_strategy_epsilon_greedy(self._q_func, self._epsilon_list[epoch])\n",
    "                # 使用该策略进行决策 \n",
    "                action_dist = e_greedy_s(current_state)\n",
    "                action = np.random.choice(self._q_func.get_actions_count(), p=action_dist)\n",
    "                # 执行这个动作，获取下一个状态      \n",
    "                reward, next_state = self._env.step(action)\n",
    "                acc_reward += reward\n",
    "                \n",
    "                if next_state is not None:\n",
    "                    self._replay_buffer.add(current_state, action, reward, next_state, 1)\n",
    "                else:\n",
    "                    self._replay_buffer.add(current_state, action, reward, current_state, 0)\n",
    "\n",
    "                current_state = next_state\n",
    "                if len(self._replay_buffer.buffer) > minimal_train_size:\n",
    "                    self.update_q_func()\n",
    "\n",
    "                if current_state is None: \n",
    "                    break\n",
    "            writer.add_scalar('reward', acc_reward, epoch)\n",
    "            writer.add_scalar('step', step_cnt, epoch)\n",
    "    \n",
    "    def update_q_func(self):\n",
    "        self.__update_count += 1\n",
    "\n",
    "        state, action, reward, next_state, weight = self._replay_buffer.sample(self._batch_size)\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        action = torch.tensor(action, dtype=torch.int64)\n",
    "        reward = torch.tensor(reward, dtype=torch.float32)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "        weight = torch.tensor(weight, dtype=int)\n",
    "\n",
    "        q_values = self._q_func(state)\n",
    "        q_values = q_values.gather(1, action.unsqueeze(1))\n",
    "\n",
    "        next_q_values = self._target_q_func(next_state)\n",
    "        target_q_values = reward + self._gamma * torch.max(next_q_values, dim=1).values * weight\n",
    "        \n",
    "        target_q_values = target_q_values.detach()\n",
    "        loss = torch.nn.functional.mse_loss(q_values, target_q_values.unsqueeze(1))\n",
    "        \n",
    "        self._optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self._optimizer.step()\n",
    "\n",
    "        if self.__update_count % self._target_update_freq == 0:\n",
    "            self._target_q_func.load_state_dict(self._q_func.state_dict())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQFuncTester():\n",
    "    def __init__(self, q_func: DeepQFunc, env: Env) -> None:\n",
    "        self._q_func = q_func\n",
    "        self._env = env\n",
    "    \n",
    "    def test(self, max_step: int):\n",
    "        init_state = self._env.reset()\n",
    "        current_state = init_state\n",
    "        acc_reward = 0\n",
    "        reward_list = []\n",
    "        # greedy_strategy = to_strategy(self._q_func)\n",
    "\n",
    "        for _ in range(max_step):\n",
    "            # action_dist = greedy_strategy(current_state)\n",
    "            # action = np.argmax(action_dist)\n",
    "            action = self._q_func.get_optimal_action(current_state)\n",
    "            \n",
    "            reward, next_state = self._env.step(action)\n",
    "            acc_reward += reward\n",
    "            reward_list.append(reward)\n",
    "            current_state = next_state\n",
    "            if current_state is None:\n",
    "                break\n",
    "    \n",
    "        print(f'Test reward: {acc_reward}')\n",
    "        print(f'Step Rewards: {reward_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action num: 2, space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "GYM_ENV_NAME = 'CartPole-v1'\n",
    "_gym_env = gym.make(GYM_ENV_NAME)\n",
    "\n",
    "action_nums, state_space = _gym_env.action_space.n, _gym_env.observation_space\n",
    "print(f'action num: {action_nums}, space: {state_space}')\n",
    "\n",
    "TRAIN_EPOCH = 1000\n",
    "HIDDEN_DIM = 128\n",
    "LEARNING_RATE = 2e-3\n",
    "GAMMA = 0.98\n",
    "\n",
    "START_EPSILON = 0.2\n",
    "END_EPSILON = 0.05\n",
    "DECAY_RATE = 0.999\n",
    "EPSILON_LIST = [max(START_EPSILON * (DECAY_RATE ** i), END_EPSILON) for i in range(TRAIN_EPOCH)]\n",
    "\n",
    "\n",
    "log_path = Path('./logs/run5_use_weight_todonestate5')\n",
    "import shutil\n",
    "if log_path.exists():\n",
    "    shutil.rmtree(log_path)\n",
    "\n",
    "q_func = DeepQFunc(state_space.shape[0], action_nums, HIDDEN_DIM)\n",
    "env = Env(_gym_env)\n",
    "q_trainer = DQFuncTrainer(\n",
    "    q_func=q_func,\n",
    "    env=env,\n",
    "    replay_buffer=ReplayBuffer(10000),\n",
    "    optimizer=optim.Adam(q_func.parameters(), lr=LEARNING_RATE),\n",
    "    batch_size=64,\n",
    "    gamma=GAMMA,\n",
    "    epsilon_list=EPSILON_LIST,\n",
    "    logger_folder=log_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 1216/5000 [07:30<23:23,  2.70it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mq_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRAIN_EPOCH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mminimal_train_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[66], line 57\u001b[0m, in \u001b[0;36mDQFuncTrainer.train\u001b[1;34m(self, epoch, max_steps, minimal_train_size)\u001b[0m\n\u001b[0;32m     55\u001b[0m current_state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replay_buffer\u001b[38;5;241m.\u001b[39mbuffer) \u001b[38;5;241m>\u001b[39m minimal_train_size:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_q_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[66], line 72\u001b[0m, in \u001b[0;36mDQFuncTrainer.update_q_func\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     69\u001b[0m next_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(next_state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     70\u001b[0m weight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(weight, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint)\n\u001b[1;32m---> 72\u001b[0m q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_q_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m next_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_q_func(next_state)\n\u001b[0;32m     74\u001b[0m target_q_values \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gamma \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(next_q_values, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m*\u001b[39m weight\n",
      "File \u001b[1;32mf:\\conda\\envs\\quant\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1545\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1544\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\conda\\envs\\quant\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1554\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1550\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1552\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1553\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1557\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[63], line 52\u001b[0m, in \u001b[0;36mDeepQFunc.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x): \n\u001b[1;32m---> 52\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fc2(x)\n",
      "File \u001b[1;32mf:\\conda\\envs\\quant\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1545\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1544\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\conda\\envs\\quant\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1554\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1550\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1552\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1553\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1557\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mf:\\conda\\envs\\quant\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "q_trainer.train(\n",
    "    epoch=TRAIN_EPOCH,\n",
    "    max_steps=1000,\n",
    "    minimal_train_size=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = DQFuncTester(q_func, Env(gym.make(GYM_ENV_NAME, render_mode='human')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test reward: 500.0\n",
      "Step Rewards: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "t.test(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "GYM_ENV_NAME = 'CartPole-v1'\n",
    "_gym_env = gym.make(GYM_ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_gym_env.observation_space.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double-Deep-Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action num: 2, space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "GYM_ENV_NAME = 'CartPole-v1'\n",
    "_gym_env = gym.make(GYM_ENV_NAME)\n",
    "\n",
    "action_nums, state_space = _gym_env.action_space.n, _gym_env.observation_space\n",
    "print(f'action num: {action_nums}, space: {state_space}')\n",
    "\n",
    "TRAIN_EPOCH = 1000\n",
    "HIDDEN_DIM = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "GAMMA = 0.98\n",
    "\n",
    "START_EPSILON = 0.2\n",
    "END_EPSILON = 0.05\n",
    "DECAY_RATE = 0.999\n",
    "EPSILON_LIST = [max(START_EPSILON * (DECAY_RATE ** i), END_EPSILON) for i in range(TRAIN_EPOCH)]\n",
    "\n",
    "\n",
    "log_path = Path('./logs/run4_doubleDeepQ_withdones')\n",
    "import shutil\n",
    "if log_path.exists():\n",
    "    shutil.rmtree(log_path)\n",
    "\n",
    "q_func = DeepQFunc(state_space.shape[0], action_nums, HIDDEN_DIM)\n",
    "target_q_func = DeepQFunc(state_space.shape[0], action_nums, HIDDEN_DIM)\n",
    "env = Env(_gym_env)\n",
    "q_trainer = DoubleQ_DQFuncTrainer(\n",
    "    q_func=q_func,\n",
    "    target_q_func=target_q_func,\n",
    "    env=env,\n",
    "    replay_buffer=ReplayBuffer(10000),\n",
    "    optimizer=optim.Adam(q_func.parameters(), lr=LEARNING_RATE),\n",
    "    batch_size=64,\n",
    "    gamma=GAMMA,\n",
    "    epsilon_list=EPSILON_LIST,\n",
    "    target_update_freq=10,\n",
    "    logger_folder=log_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]C:\\Users\\29000\\AppData\\Local\\Temp\\ipykernel_19184\\1330176196.py:60: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  out = self.forward(torch.tensor([state]))\n",
      "100%|██████████| 1000/1000 [05:46<00:00,  2.88it/s]\n"
     ]
    }
   ],
   "source": [
    "q_trainer.train(\n",
    "    epoch=TRAIN_EPOCH,\n",
    "    max_steps=1000,\n",
    "    minimal_train_size=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 问题的关键在于每个epoch结束的时刻，那个state-action-reward 特别重要，如果丢失，则Q函数无法工作！！！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
