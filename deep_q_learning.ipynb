{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 考虑连续State空间、离散Action 空间的Q函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Callable, List, Tuple, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym\n",
    "from tensorboardX import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "\n",
    "State = int\n",
    "# 表示状态特征向量的维度\n",
    "StateDim = int \n",
    "\n",
    "Action = int\n",
    "Reward = float\n",
    "ActionProbDistribution = List[float]\n",
    "\n",
    "class AbstractQFunc():\n",
    "    def get_value(self, state: State, action: Action) -> float:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def get_action_distribute(self, state: State) -> ActionProbDistribution:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_actions_count(self) -> int:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def set_value(self, state: State, action: Action, value: float) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class DeepQFunc(AbstractQFunc, torch.nn.Module):\n",
    "    def __init__(self, state_dim: int, action_nums: int) -> None:\n",
    "        # here use full-connect layer to represent Q function\n",
    "        super().__init__() \n",
    "        self._state_dims = state_dim \n",
    "        self._action_nums = action_nums\n",
    "        \n",
    "        self._fc1 = torch.nn.Linear(state_dim, 128)\n",
    "        self._fc2 = torch.nn.Linear(128, action_nums)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = torch.nn.functional.relu(self._fc1(x))\n",
    "        return self._fc2(x)\n",
    "        \n",
    "\n",
    "    def set_value(self, state: State, action: Action, value: float) -> None:\n",
    "        self._q_table[state][action] = value\n",
    "\n",
    "    def get_action_distribute(self, state: State) -> ActionProbDistribution:\n",
    "        return self._q_table[state]\n",
    "\n",
    "    def get_actions_count(self) -> int:\n",
    "        return self._action_nums"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
